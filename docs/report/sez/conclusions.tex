\section{Conclusions}
The project made us understand how the theory seen during the course was strictly correlated with the implementation of a real neural network library. Thanks to MONK's dataset we discovered how really the regularization techniques prevent overfitting controlling model complexity. We understood the importance of choosing a good model and how to do it using grid search and k-cross validation. The hyperparameter tuning was also the most difficult phase with the debugging and testing of the backpropagation algorithm. This experience thought us the foundations of learning algorithms on neural networks on which many machine learning frameworks are based today. The experience of working in a team has allowed us to refine our knowledge about how real work experience is done and helped us not to break down during development.



\begin{thebibliography}{9}

	\bibitem{numerical} 
	Jorge Nocedal  Stephen J. Wright.
	\textit{Numerical Optimization}. Springer, (2nd Edition, 2006).

	\bibitem{PaperL-BFQS} 
	Dong C. Liu and Jorge Nocedal.
	\textit{On the limited memory BFGS method for large scale optimization}. Mathematical Programming 45 (1989), pp. 503-528.
	\\\texttt{https://people.sc.fsu.edu/~inavon/5420a/liu89limited.pdf}
		
	\bibitem{gradientProof} 
	Léon Bottou, Frank E. Curtis, and Jorge Nocedal.
	\textit{Optimization methods for largescale machine learning}. 
	[\textit{ Analyses of Stochastic Gradient Methods}]. Chapter 4.
	2016.
	
	\bibitem{armadillo} 		
	Conrad Sanderson and Ryan Curtin. 
	\textit{Armadillo: a template-based C++ library for linear algebra}. 
	\\\texttt{http://arma.sourceforge.net/armadillo\_joss\_2016.pdf}

	
	\bibitem{haykin} 
	Simon Haykin. 
	\textit{Neural Networks and Learning Machines}. 
	Prentice-Hall, (3rd Edition, 2008).


	\bibitem{mitchell} 
	T. M. Mitchell. 
	\textit{Machine learning}. 
	McGraw-Hill, 1997.
	
	\bibitem{goodfellow} 
	I. Goodfellow, Y. Bengio, A. Courville. 
	\textit{Deep Learning}. 
	MIT Press,  2016.
	
	\bibitem{bert03} 
	D.P. Bertsekas.
	\textit{Nonlinear Programming}. 
	Second edition. Athena Scientific, 2003.
	
	\bibitem{PaperBM} 
	A. Frangioni.
	\textit{Standard Bundle Methods: Untrusted Models and Duality}.
	\\\texttt{http://pages.di.unipi.it/frangio/abstracts.html\#NDOB18}
	
	\bibitem{NonOpt} 
	Andrzej Ruszczyński.
	\textit{Nonlinear Optimization}. Princeton University Press. 
	
	\bibitem{CBM} 
	Y. Du and A. Ruszczyński.
	\textit{Rate of Convergence of the Bundle Method}. Journal  of  Optimization Theory and Applications.
	\\\texttt{https://arxiv.org/pdf/1609.00842.pdf}

	\bibitem{NCBM} 
	Dominikus Noll.
	\textit{Bundle method for non-convex minimization with inexactsubgradients and function values}. 
	\\\texttt{http://www.optimization-online.org/DB\_FILE/2012/02/3341.pdf}

	\bibitem{monk} 
	S.B. Thrun, J. Bala, E. Boloederon and I. Bratko.
	\textit{The MONK's Problems}. 
	[\textit{A performance comparison of different learning algoritms}]. Chapter 9.
	Carnegie Mellon University CMU-CS-91-197, 1991.
	\\\texttt{https://pdfs.semanticscholar.org/94c0/418c4bd9d719e1203acfd42741ebbd343073.pdf}
	

 

	
\end{thebibliography}



