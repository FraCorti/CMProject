\section{Introduction}
At the beginning we provide a short description of the problem. Next, we talk about the implemented method to find the gradient and the activation function used in the experiments. In the end, we give some information about the used regularization method.
\subsection{Neural network}
Let \textit{M} be a neural network with a specific topology. The main goal of this project was to learn and develop three different optimization methods for \textit{M}. We exploit:
\begin{itemize}
	\label{learningAlgorithm}
	\item Standard momentum descent approach;
	\item L-BFGS algorithm of the class of limited-memory quasi-Newton methods for L2 regularization;
	\item Proximal Bundle Method algorithm of the class of bundle methods for L1 regularization;
\end{itemize}
\subsection{Backpropagation}
The backpropagation algorithm can be divided into two phases (as mentioned in \cite{backpropagation}):
\begin{enumerate}
	\item Compute the network's gradient that is the derivative of the cost function
	$\nabla_{\theta} J(\theta)$, with $\theta$ representing the ANN's parameters (weights and bias). The algorithm used to compute the gradient is the \textit{back-propagation} described by
	algorithm \ref{alg:fp} and \ref{alg:bp};
	\item Use the knowledge of the gradient to do the next step using one of the optimizers chosen;
\end{enumerate}
The computation of the gradient is divided in two parts that are \textit{forward} and \textit{backward}. In the \textit{forward} phase the input matrix \textit{x} flow through the network ending in the output layer that computes the output \textit{h(x)}. This later is compared with the desired vector values $\widehat{y}$:
\begin{algorithm}[H]
	\caption{Forward propagation}
	\label{alg:fp}
	\begin{algorithmic}[1]
		\Procedure{Forward propagation}{}
		\State $\mathbf{h}_{0} = \mathbf{x}$
		\For{$k = 1, \ldots, l$}
		\State $\mathbf{a}_{k} = \mathbf{b}_{k} + \mathbf{W}_{k}\mathbf{h}_{k - 1}$
		\State $\mathbf{h}_{k} = f(\mathbf{a}_{k})$
		\EndFor
		\State $\mathbf{h(x)} = \mathbf{h}_{l}$
		\State $J = L(\mathbf{h(x)}, \mathbf{y}) + \lambda \Omega(\theta)$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
In our case, $J$ is the \textit{Mean Squared Error} function. Since the neural network is a composition of functions the Chain Rule is used to compute the partial derivative of the weights that compose it. 

\begin{algorithm}[H]
	\caption{Backward computation}
	\label{alg:bp}
	\begin{algorithmic}[2]
		\Procedure{Backward propagation}{}
		\State $\mathbf{g} \leftarrow \nabla_{\hat{\mathbf{y}}}J = \nabla_{\hat{\mathbf{y}}}
		L(\mathbf{h(x)}, \mathbf{y})$
		\For{$k = l, l - 1, \ldots, 1$}
		\State $\mathbf{g} \leftarrow \nabla_{\mathbf{a}_{k}}J = \mathbf{g} \ \odot \
		f'(\mathbf{a}_{k})$
		\State $\nabla_{\mathbf{b}_{k}}J = \mathbf{g} \ + \ \lambda \nabla_{\mathbf{b}_{k}}
		\Omega(\theta)$
		\State $\nabla_{\mathbf{W}_{k}}J = \mathbf{g}\mathbf{h}_{k - 1}^{T} \ + \ \lambda
		\nabla_{\mathbf{W}_{k}} \Omega(\theta)$
		\State $\mathbf{g} = \nabla_{\mathbf{h}_{k - 1}}J = \mathbf{W}_{k}^{T}\mathbf{g}$
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\subsection{Activation function}
\label{activationFunction}
The activation function of a node defines its output given an input or a set of inputs. 
Properties of this function are:
\begin{itemize}
	\item Nonlinear;
	\item Range;
	\item Continuously differentiable;
	\item Monotonic;
	\item Smooth functions;
	\item Approximates identity near the origin;
\end{itemize}

For the aim of our project, we chose to use two different activation function:
\begin{itemize}
	\label{sigmoid}
	\item Sigmoid (or standard logistic function):
	\begin{itemize}
		\item It is defined between (0,1); 		
			\begin{align*}
			&f(x) = \sigma(x) = \frac{1}{1 + e^{-x}} \\
			&f'(x) = \sigma'(x) = f(x)(1 - f(x)),
			\end{align*} 
	\end{itemize}
	\item TanH;
	\label{tanH}
	\begin{itemize}
		\item It is defined between (-1,1); 		
		\begin{align*}
		&f(x) = \sigma(x) = \frac{e^{x}-e^{-x}}{e^{x} + e^{-x}} \\
		&f'(x) = \sigma'(x) = 1 - f(x)^{2},
		\end{align*} 
	\end{itemize}
\end{itemize}
The purpose of the nonlinearity is to ensure that the neural network is a universal function approximator.
\subsection{Loss Function}
\label{Loss:Mse}
The \textit{Loss Function} is a function used to evaluate the performance of a model, given the $h(x)$ vector compute by the network and the desired vector values $\widehat{y}$ measures the average of the squares of the errors. There are several \textit{Loss Function} used in machine learning algorithms but we are going to focus on the \textit{Mean Squared Error} (MSE). This is obtained by the formula: 	
\begin{equation}
MSE = \frac{1}{n} \sum_{i=1}^n (h(x) - \widehat{y})_{i}^2
\end{equation}
where $n$ represents the number of sample input data we passed into the model. 
The \textit{Loss Function} can be represented as a composition of the Euclidean norm and the quadratic function
\begin{equation}
MSE = \frac{1}{n} \parallel h(x) - \widehat{y} \parallel_{2}^2  
\end{equation}

Moreover, the training phase of a supervised machine learning algorithm can be seen as an optimization (in our case minimization) of the \textit{Loss Function} by altering the weights of the network $w$.   
Since the purpose of neural networks is to build models that fit data, we want to minimize the \textit{Loss Function} to have a good prediction on unseen data. This minimization process is done through optimization algorithms but to minimize the \textit{Loss Function} it must have certain properties.	
The gradient for the MSE concerning $x_{i}$ is defined as:
\begin{equation}
\nabla_{x_{i}}MSE(x, \hat{y})= 2*(h(x)-\hat{y})_{i} * h'(x)
\end{equation}
The $\nabla w $ is equal to:
\begin{equation}
\label{derivationGradient}
\nabla w= -\frac{\partial MSE(x, \hat{y})}{\partial w} \, =\, - \sum_{i=1}^n \frac{\partial MSE(x, \hat{y})_i}{\partial w} = \sum_{i=1}^n -\frac{\partial MSE(x, \hat{y})_i}{\partial w} = \sum_{i=1}^n \nabla_{i} w
\end{equation}
The $\nabla_{i} w $ for a generic \textit{t} is equal to:
\begin{equation*}
\nabla_{i} w_{t} \, = \, -\frac{\partial MSE(x, \hat{y})_i}{\partial w_{t}} \, = \, -\frac{\partial MSE(x, \hat{y})_i}{\partial o_{t}} * \frac{\partial o_{t}}{\partial net_{t}}*\frac{\partial net_{t}}{\partial w_{t}}
\end{equation*}
where $o_{t} \, = \, f_{t}(net_{t})$, $f_{t}$ is the activation function at layer \textit{t}, $net_{t} \, = \, \sum_{i=1}^n w_{t,i}*o_{t-1,i}$ and $ o_{0} $ are the inputs.
\\
So, $\frac{\partial net_{t}}{\partial w_{t,i}}$ is equal to:
\begin{equation*}
\frac{\partial net_{t}}{\partial w_{t}} \, = \, \frac{\partial\sum_{r=1}^n  w_{t,r}*o_{t-1,r}}{\partial w_{t,i}} \, = \, o_{t-1,i}
\end{equation*}
The term $\frac{\partial o_{t}}{\partial net_{t}}$ is equal to:
\begin{equation*}
 \frac{\partial o_{t}}{\partial net_{t}} \, = \, f'(net_{t})
\end{equation*}
We define:
\begin{equation*}
\delta_{t} \,=\, -\frac{\partial MSE(x, \hat{y})_i}{\partial o_{t}} * \frac{\partial o_{t}}{\partial net_{t}}
\end{equation*}
Now we have to study two different case for $\frac{\partial MSE(x, \hat{y})_i}{\partial o_{t}}$, when t is the output layer and when t is a hidden layer.
\\
Case t = k (where k is the last layer):
\begin{equation*}
\frac{\partial MSE(x, \hat{y})_i}{\partial o_{k}} \, = \, -\frac{1}{2} * \frac{\sum_{r=1}^n \partial((h(x) - \widehat{y})_{r}^2)}{\partial o_{k}} \, = \, - \frac{\sum_{r=1}^n (h(x)-\hat{y})_{r} * h'(x) \partial((h(x) - \widehat{y})_{r})}{\partial o_{k}} \, = \, (h(x)-\hat{y}) * h'(x)
\end{equation*}
So, $\delta_{t}$ is equal to:
\begin{equation*}
\delta_{k} \, = \, (h(x)-\hat{y}) * h'(x) * f'(net_{k})
\end{equation*}

Case t = j (where j is a hidden layer):
\begin{equation*}
\frac{\partial MSE(x, \hat{y})_i}{\partial o_{j}} \,
 = \, -\frac{1}{2} * \sum_{r=j}^k\frac{ \partial((h(x) - \widehat{y})_{r}^2)}{\partial o_{r}} \, 
= \, -\frac{1}{2} * \sum_{r=j}^k\frac{ \partial((h(x) - \widehat{y})_{r}^2)}{\partial o_{r}} * \frac{\partial o_{r}}{\partial net_{r}} * \frac{\partial net_{r}}{\partial o_{j}}
\end{equation*}

\begin{equation}
\label{retropagation}
= \, \sum_{k} \delta_{k} * w_{k,j}
\end{equation}
Where:
\begin{equation*}
\frac{\partial net_{r}}{\partial o_{j}} \, = \, \frac{\sum_{r=1}^n\partial w_{k,r}*o_{r}}{\partial o_{j}} \, = \,  w_{k,j}
\end{equation*}
After the derivation we can state that in the output layer $k$ the gradient is defined as: 
\begin{equation}
\label{partialOutput}
\nabla w_{k} \, = \, (h(x)-\hat{y}) * h'(x) * f'(net_{k})  * o_{t-1,i}
\end{equation}
and in the hidden layer $j$ is defined as:
\begin{equation}
\label{partialHidden}
\nabla w_{j} \, = \, (\sum_{k} \delta_{k} * w_{k,j}) * f'(net_{j}) * o_{t-1,i}
\end{equation}

\subsubsection{Loss Function properties}
\label{LF:Properties}
In an optimization problem given $X$ any set and $f: X \rightarrow \mathbb{R}$ any function we want
\begin{equation}
(P) \quad f_{*} = min \{f(x) : x \in X\}
\end{equation}
where X is the feasible region, f is the objective function and $v(P) = f_{*}$ is the optimal value. In our case $X \subseteq \mathbb{R}^{n}$ and we want to be sure that exists an optimal solution.

So we want to find an optimal solution: $x_{*} \in X  \textnormal{ such that } f(x_{*}) = f_{*}$ but this can be impossible for many reasons. For the \textit{Bolzano-Weierstrass} theorem to ensure that our function has an optimal solution we need that $X \subseteq \mathbb{R}^{n}$ is compact and $f$ is continuous (or lower semi-continuous) and differentiable. 

\begin{itemize}
	\item \textbf{Continuity}: A function $f$ is \textit{Lipschitz continuous} on its domain $S$ if $\exists L>0$ such that
	\begin{equation}
	|f(x)-f(y)| \leq L\parallel x-y\parallel \quad \forall x,y \in S,  
	\end{equation} 
	more formally a function is \textit{Locally Lipschitz Continuous} at $x$ if $\exists \varepsilon >0 \textnormal{ s.t } S \in \beta(x,\varepsilon) $
	and it is \textit{Global Lipschitz Continuous} if $f$ is \textit{Locally Lipschitz Continuous} on all the space S, in our case $R^n$.
	Since neural networks are a series of function composition: 
	\begin{equation}
	h(x) = \phi_{k}(b_{k} + \sum_{j}w_{kj} \phi_{j}(b_{j} + \sum_{i}w_{ji}x))
	\end{equation}
	where $x$ is the input and $\phi_{i}$ is an activation function. 
	From the Theorem 12.6 of \cite{MFD}: Let $f_{1}$ be Lipschitz continuous on a set $I_{1}$ with Lipschitz constant $L_{1}$ and $f_{2}$ be Lipschitz continuous on $I_{2}$ with Lipschitz constant $L_{2}$ such that $f_{1}(I_{1}) \subset I_{2}$ . Then the composite function= $f_{1}\circ f_{2}$ is Lipschitz continuous on $I_{1}$ with Lipschitz constant $L1*L2$. 
	\\
	In our case MSE is a composition of the Euclidean norm, that is quadratic and Lipschitz continuous function, and the output function, that is the composition between the used activation functions that are Lipschitz continuous. To ensure that the Theorem 12.6 of \cite{MFD} is valid in our case, the computed value of the output function have to live in a bounded set. As mentioned in \S \ref{activationFunction} each layer can use a \textit{sigmoid} that bounds the output between (0,1) or a \textit{tanH} that bounds the output  between (-1,+1) as the activation function. Since every layer's output is bounded and the Euclidean norm receive an input in a bounded interval then for Theorem 12.6 of \cite{MFD} the MSE is a Lipschitz continuous function.	
	\item  \textbf{Differentiability}:  the network uses \textit{sigmoid} (\S \ref{sigmoid}) and \textit{hyperbolic tangent} (\S \ref{tanH}) as activation functions. These functions are continuous and twice differentiable with bounded Lipschitz continuous derivative. So \textit{Mean Squared Error} is a differentiable function since the network is a composition of continuously differentiable functions. 
   Moreover, in our case the gradient has to be Lipschitz continuous for the Momentum Descent Approach, instead for the L-BFGS and Proximal Bundle Method we do not need this property to converge to local minima. This implies the following discussion. The Theorem 12.4 of \cite{MFD} states that if $ f_{1}$ and $ f_{2}$ are Lipschitz continuous on a bounded interval I then $ f_{1}*f_{2}$ is Lipschitz continuous on I. For this theorem the gradient of our loss function is Lipschitz continuous if we use only activation functions with Lipschitz derivative and we restrict the weights to a bounded set. Also in derivation \ref{derivationGradient}, which leads to \ref{partialOutput} and \ref{partialHidden}, we can see that the gradient is bounded if the weights are restricted in a bounded set because it is a composition between values (weights and activation function output) and derivatives of the activation function that are bounded. The weights of the network are initialized using a uniform distribution, taken in the range $[1,-1]$. We can observe that the weights are in a compact set if the algorithm guarantees a monotone sequence of objective values (i.e. $f(x_{i})$) when it is converging to local minima. The Momentum Descent Approach produces a monotone sequence of objective values when the step length (also known as learning rate) is sufficiently small (this is verified in \cite{numerical} \S 2.2 paragraph "Search directions for line search methods"). We define the objective value as $f(w) = Loss(w) + \lambda || w || $ where $Loss(w)$ is a non negative function, $\lambda > 0$ and  $|| w ||$ is the regularization term. We can state that $f(w_{0}) = Loss(w_{0}) + \lambda || w_{0} || $ where $w_{0}$ is the initial point. We can assert that if the algorithm guarantees a monotonic sequence of objective values (that is our case since we use Momentum Descent Approach with a sufficiently small step length) then $f(w_{i}) = Loss(w_{i}) + \lambda || w_{i} || \leq f(w_{0}) \, \forall i$ . It follows that $|| w_{i} || \leq  f(w_{0}) - Loss(w_{i}) \leq f(w_{0})$ because $Loss(w_{i}) \geq 0$. For this reason $w_{i}$ lives in a compact set. Unfortunately, the Nesterov Descent Approach, that we have implemented, do not guarantee the production of a monotone sequence of objective values. But, we find out during the experimental phase that in the problems that we choose, the weights of the Nesterov Descent Approach lives in a compact set and this all that we need. Therefore, after analysing the results of the experimental phase, we can state that in our problems the gradient is Lipschitz continuous also for the Nesterov Descent Approach.
	\item \textbf{Convexity}: all the functions used as activation function ($sigmoid$ and $tanH$) are not convex functions. Since our MSE (\textit{Loss Function}) is obtained combining these not convex functions MSE is not convex. 
	\label{LF:convexity}
\end{itemize}

\subsection{Regularization}
 In machine learning, the regularization is used to ensure a trade-off between accuracy in the training set and complexity of the model.
 We implemented and used two types of regularization, L1 and L2. They are implemented adding at the objective Loss Function a penalty term multiplied by a lambda parameter.
\begin{align*}
	{\mathbf{W} \in \mathbb{R}^n} {\ \mathit{L}(\mathbf{W}) + \lambda\Omega(\mathbf{W})}{}{}
	\label{eq:reg}
\end{align*}
 
\paragraph*{L1 regularization}
Usually named as Lasso regression, defined as:
$\Omega(\textbf{W}) = \sum_{i=1}^{k} |w_i| = \|\textbf{W}\|_1$.
\paragraph*{L2 regularization}
Usually named as Ridge regression, defined as:
$\Omega(\textbf{W}) = \sum_{i=1}^{k}w_i^2 = \|\textbf{W}\|_2^2$. 
 
 
%The first section of your report should contain a description of the problem and the methods that you plan to use.This is intended just as a brief recall, to introduce some notation and specify which variants of the methods you are planning to use exactly. Discuss the reasons behind the choices you make (the one you can make, that is, since several of them will be dictated by the statement of the project and cannot be questioned).Your target audience should be someone who is already sufficiently familiar with the content of the course. This is not the place to show your knowledge and repeat a large part of the theory: we are sure that you all can do that,1
%2 Structure of your report2given enough time, books, slides, and internet bandwidth. A more in-depth mathematical part is expected in the next stage.In case adapting the algorithm to your problem requires some further mathematical derivation (example: developingan exact line search for your problem, when possible, or adapting an algorithm to deal more efficiently with the special structure of your problem), you are supposed to discuss it here with all the necessary mathematical detail. You are advised to send us a version of this section by e-mail as soon as it is done, so that we can catch misunderstandings as soon as possible and minimize the amount of work wasted. Note that we do not want to see code at this point: that would be premature to produce (for you) and unnecessarily complicated to read (for us).