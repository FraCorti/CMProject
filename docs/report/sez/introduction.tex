\section{Introduction}
At the beginning we provide a short description of the problem. Next, we talk about the implemented method to find the gradient and the activation function used in the experiments. At the end, we give some information about the used regularization method.
\subsection{Neural network}
Let \textit{M} be a neural network with a specific topology. The main goal of this project was to learn and develop three different optimization methods for \textit{M}. We exploit:
\begin{itemize}
	\item Standard momentum descent approach;
	\item L-BFGS algorithm of the class of limited-memory quasi-Newton methods for L2 regularization;
	\item Proximal Bundle Method algorithm of the class of bundle methods for L1 regularization;
\end{itemize}
\subsection{Backpropagation}
The backpropagation algorithm can be divided in two phases (as mentioned in \cite{backpropagation}):
\begin{enumerate}
	\item Compute the network's gradient that is the derivative of the cost function
	$\nabla_{\theta} J(\theta)$, with $\theta$ representing the ANN's parameters (weights and bias). The algorithm used to compute the gradient is the \textit{back-propagation} described by
	algorithm \ref{alg:fp} and \ref{alg:bp};
	\item Use the knowledge of the gradient to do the next step using one of the optimizers chosen;
\end{enumerate}
The computation of the gradient is divided in two parts that are \textit{forward} and \textit{backward}. In the \textit{forward} phase the input matrix \textit{x} flow through the network ending in the output layer that compute the output \textit{h(x)}. This later, is compared with the desired vector values $\widehat{y}$:
\begin{algorithm}[H]
	\caption{Forward propagation}
	\label{alg:fp}
	\begin{algorithmic}[1]
		\Procedure{Forward propagation}{}
		\State $\mathbf{h}_{0} = \mathbf{x}$
		\For{$k = 1, \ldots, l$}
		\State $\mathbf{a}_{k} = \mathbf{b}_{k} + \mathbf{W}_{k}\mathbf{h}_{k - 1}$
		\State $\mathbf{h}_{k} = f(\mathbf{a}_{k})$
		\EndFor
		\State $\mathbf{h(x)} = \mathbf{h}_{l}$
		\State $J = L(\mathbf{h(x)}, \mathbf{y}) + \lambda \Omega(\theta)$
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
In our case $J$ is the \textit{Mean Squared Error} function. Since the neural network is a composition of functions the Chain Rule is used to compute the partial derivative of the weights that compose it. 

\begin{algorithm}[H]
	\caption{Backward computation}
	\label{alg:bp}
	\begin{algorithmic}[2]
		\Procedure{Backward propagation}{}
		\State $\mathbf{g} \leftarrow \nabla_{\hat{\mathbf{y}}}J = \nabla_{\hat{\mathbf{y}}}
		L(\mathbf{h(x)}, \mathbf{y})$
		\For{$k = l, l - 1, \ldots, 1$}
		\State $\mathbf{g} \leftarrow \nabla_{\mathbf{a}_{k}}J = \mathbf{g} \ \odot \
		f'(\mathbf{a}_{k})$
		\State $\nabla_{\mathbf{b}_{k}}J = \mathbf{g} \ + \ \lambda \nabla_{\mathbf{b}_{k}}
		\Omega(\theta)$
		\State $\nabla_{\mathbf{W}_{k}}J = \mathbf{g}\mathbf{h}_{k - 1}^{T} \ + \ \lambda
		\nabla_{\mathbf{W}_{k}} \Omega(\theta)$
		\State $\mathbf{g} = \nabla_{\mathbf{h}_{k - 1}}J = \mathbf{W}_{k}^{T}\mathbf{g}$
		\EndFor
		\EndProcedure
	\end{algorithmic}
\end{algorithm}
\subsection{Activation function}
\label{activationFunction}
The activation function of a node defines the output of that node given an input or set of inputs. 
Properties of this function can be:
\begin{itemize}
	\item Nonlinear;
	\item Range;
	\item Continuously differentiable;
	\item Monotonic;
	\item Smooth functions with a monotonic derivative;
	\item Approximates identity near the origin;
\end{itemize}

For the aim of our project, we chose to use two different activation function:
\begin{itemize}
	\label{sigmoid}
	\item Sigmoid (or standard logistic function):
	\begin{itemize}
		\item It is defined between (0,1); 		
			\begin{align*}
			&f(x) = \sigma(x) = \frac{1}{1 + e^{-x}} \\
			&f'(x) = \sigma'(x) = f(x)(1 - f(x)),
			\end{align*} 
	\end{itemize}
	\item TanH;
	\label{tanH}
	\begin{itemize}
		\item It is defined between (-1,1); 		
		\begin{align*}
		&f(x) = \sigma(x) = \frac{e^{x}-e^{-x}}{e^{x} + e^{-x}} \\
		&f'(x) = \sigma'(x) = 1 - f(x)^{2},
		\end{align*} 
	\end{itemize}
\end{itemize}
The purpose of the nonlinearity is to ensure that the neural network is a universal function approximator.
\subsection{Loss Function}
\label{Loss:Mse}
The \textit{Loss Function} is a function used to evaluate the performance of a model, given the $h(x)$ vector compute by the network and the desired vector values $\widehat{y}$ measures the average of the squares of the errors. There are several \textit{Loss Function} used in machine learning algorithms but we are going to focus on the \textit{Mean Squared Error} (MSE). This is obtained by the formula: 	
\begin{equation}
MSE = \frac{1}{n} \sum_{i=1}^n (h(x) - \widehat{y})_{i}^2
\end{equation}
where $n$ represents the number of sample input data we passed into in the model. 
The \textit{Loss Function} can be represented as a composition of the Euclidean norm and the quadratic function
\begin{equation}
MSE = \frac{1}{n} \parallel h(x) - \widehat{y} \parallel_{2}^2  
\end{equation}

Moreover the train phase of a supervised machine learning algorithm can be seen as an optimization (in our case minimization) of the \textit{Loss Function} by altering the weights of the network $w$.   
Since the purpose of neural networks is to build models that fits data, we want to minimize the \textit{Loss Function} in order to have good prediction on unseen data. This minimization process is done through optimization algorithms but to minimize the \textit{Loss Function} it must have certain properties.	
The gradient for the MSE with respect to $x_{i}$ is defined as:
\begin{equation}
\nabla_{x_{i}}MSE(x, \hat{y})= 2*(h(x)-\hat{y})_{i} * h'(x)
\end{equation}
The $\nabla w $ is equal to:
\begin{equation*}
\nabla w= -\frac{\partial MSE(x, \hat{y})}{\partial w} \, =\, - \sum_{i=1}^n \frac{\partial MSE(x, \hat{y})_i}{\partial w} = \sum_{i=1}^n -\frac{\partial MSE(x, \hat{y})_i}{\partial w} = \sum_{i=1}^n \nabla_{i} w
\end{equation*}
The $\nabla_{i} w $ for a generic \textit{t} is equal to:
\begin{equation*}
\nabla_{i} w_{t} \, = \, -\frac{\partial MSE(x, \hat{y})_i}{\partial w_{t}} \, = \, -\frac{\partial MSE(x, \hat{y})_i}{\partial o_{t}} * \frac{\partial o_{t}}{\partial net_{t}}*\frac{\partial net_{t}}{\partial w_{t}}
\end{equation*}
where $o_{t} \, = \, f_{t}(net_{t})$, $f_{t}$ is the activation function at layer \textit{t}, $net_{t} \, = \, \sum_{i=1}^n w_{t,i}*o_{t-1,i}$ and $ o_{0} $ are the inputs.
\\
So, $\frac{\partial net_{t}}{\partial w_{t,i}}$ is equal to:
\begin{equation*}
\frac{\partial net_{t}}{\partial w_{t}} \, = \, \frac{\partial\sum_{r=1}^n  w_{t,r}*o_{t-1,r}}{\partial w_{t,i}} \, = \, o_{t-1,i}
\end{equation*}
The term $\frac{\partial o_{t}}{\partial net_{t}}$ is equal to:
\begin{equation*}
 \frac{\partial o_{t}}{\partial net_{t}} \, = \, f'(net_{t})
\end{equation*}
We define:
\begin{equation*}
\delta_{t} \,=\, -\frac{\partial MSE(x, \hat{y})_i}{\partial o_{t}} * \frac{\partial o_{t}}{\partial net_{t}}
\end{equation*}
Now we have to study two different case for $\frac{\partial MSE(x, \hat{y})_i}{\partial o_{t}}$, when the t is the output layer and when t is a hidden layer.
\\
Case t = k (where k is the last layer):
\begin{equation*}
\frac{\partial MSE(x, \hat{y})_i}{\partial o_{k}} \, = \, -\frac{1}{2} * \frac{\sum_{r=1}^n \partial((h(x) - \widehat{y})_{r}^2)}{\partial o_{k}} \, = \, - \frac{\sum_{r=1}^n (h(x)-\hat{y})_{r} * h'(x) \partial((h(x) - \widehat{y})_{r})}{\partial o_{k}} \, = \, (h(x)-\hat{y}) * h'(x)
\end{equation*}
So, $\delta_{t}$ is equal to:
\begin{equation*}
\delta_{k} \, = \, (h(x)-\hat{y}) * h'(x) * f'(net_{k})
\end{equation*}

Case t = j (where j is a hidden layer):
\begin{equation*}
\frac{\partial MSE(x, \hat{y})_i}{\partial o_{j}} \,
 = \, -\frac{1}{2} * \sum_{r=j}^k\frac{ \partial((h(x) - \widehat{y})_{r}^2)}{\partial o_{r}} \, 
= \, -\frac{1}{2} * \sum_{r=j}^k\frac{ \partial((h(x) - \widehat{y})_{r}^2)}{\partial o_{r}} * \frac{\partial o_{r}}{\partial net_{r}} * \frac{\partial net_{r}}{\partial o_{j}}
\end{equation*}

\begin{equation*}
= \, \sum_{k} \delta_{k} * w_{k,j}
\end{equation*}
Where:
\begin{equation*}
\frac{\partial net_{r}}{\partial o_{j}} \, = \, \frac{\sum_{r=1}^n\partial w_{k,r}*o_{r}}{\partial o_{j}} \, = \,  w_{k,j}
\end{equation*}

\subsubsection{Loss Function properties}
\label{LF:Properties}
In an optimization problem given $X$ any set and $f: X \rightarrow \mathbb{R}$ any function we want
\begin{equation}
(P) \quad f_{*} = min \{f(x) : x \in X\}
\end{equation}
where X is the feasible region, f is the objective function and $v(P) = f_{*}$ is the optimal value. In our case $X \subseteq \mathbb{R}^{n}$ and we want to be sure that exist an optimal solution.

So we want to find any optimal solution: $x_{*} \in X  \textnormal{ such that } f(x_{*}) = f_{*}$ but this can be impossible for many reasons. For the \textit{Weierstrass} theorem to ensure that our function has an optimal solution we need that $X \subseteq \mathbb{R}^{n}$ is compact and $f$ is continuous (or lower semi-continuous) and differentiable. 

\begin{itemize}
	\item \textbf{Continuity}: A function $f$ is \textit{ Lipschitz continuous} on its domain $S$ if $\exists L>0$ such that
	\begin{equation}
	|f(x)-f(y)| \leq L\parallel x-y\parallel \quad \forall x,y \in S,  
	\end{equation} 
	more formally a function is  \textit{Locally Lipschitz Continuous} at $x$ if $\exists \varepsilon >0 \textnormal{ s.t } S \in \beta(x,\varepsilon) $
	and it is \textit{Global Lipschitz Continuous} if $f$ is \textit{Locally Lipschitz Continuous} on all the space S, in our case $R^n$.
	Since neural networks are a series of function composition: 
	\begin{equation}
	h(x) = \phi_{k}(b_{k} + \sum_{j}w_{kj} \phi_{j}(b_{j} + \sum_{i}w_{ji}x))
	\end{equation}
	where $x$ is the input and $\phi_{i}$ is an activation function. MSE is a composition of the Euclidean norm that is quadratic, and the output function, so the composition between the used activation functions. To ensure it's Lipschitz continuity we have to restrict its domain to a bounded set, but as mentioned in section \ref{activationFunction} each layer can use a \textit{sigmoid} that bounds the output between (0,1) or \textit{tanH} that bounds the output  between (-1,+1) as activation function. Since in our case every layer's output is bounded the MSE is a continuous function.	
	\item  \textbf{Differentiability}:  the network use \textit{sigmoid} (section \ref{sigmoid}) and \textit{hyperbolic tangent} (section \ref{tanH}) as activation functions. These functions are continuous and twice differentiable with bounded Lipschitz continuous derivative. So \textit{Mean Squared Error} is a differentiable function since the network is a composition of continuously differentiable functions. Moreover, the gradient of our loss function is Lipschitz continuous if we restrict the weights to a bounded set and we use activation functions with Lipschitz derivative. The weights of the network are initialized using a uniform distribution, taken in the range $[1,-1]$.
	\item \textbf{Convexity}: all the functions used as activated function ($sigmoid$ and $tanH$) are not convex functions. Since our MSE \textit{Loss Function} is obtained combining these not convex functions MSE is not convex. Moreover, the properties of the sigmoid and hyperbolic tangent activation function described in \S \ref{activationFunction} allow the gradient of our loss functions to be Lipschitz continuous.
\end{itemize}

\subsection{Regularization}
 In machine learning, is used to insure a trade off between accuracy in training set and complexity of the model.
 We implemented and used two type of regularization, L1 and L2. They are implemented adding at the objective Loss Function a penalty term multiplied by a lambda parameter.
\begin{align*}
	{\mathbf{W} \in \mathbb{R}^n} {\ \mathit{L}(\mathbf{W}) + \lambda\Omega(\mathbf{W})}{}{}
	\label{eq:reg}
\end{align*}
 
\paragraph*{L1 regularization}
Usually named as Lasso regression it was defined as follow:
$\Omega(\textbf{W}) = \sum_{i=1}^{k} |w_i| = \|\textbf{W}\|_1$.
\paragraph*{L2 regularization}
Usually named as Ridge regression it was defined as follow:
$\Omega(\textbf{W}) = \sum_{i=1}^{k}w_i^2 = \|\textbf{W}\|_2^2$. 
 
 
%The first section of your report should contain a description of the problem and the methods that you plan to use.This is intended just as a brief recall, to introduce some notation and specify which variants of the methods you are planning to use exactly. Discuss the reasons behind the choices you make (the one you can make, that is, since several of them will be dictated by the statement of the project and cannot be questioned).Your target audience should be someone who is already sufficiently familiar with the content of the course. This is not the place to show your knowledge and repeat a large part of the theory: we are sure that you all can do that,1
%2 Structure of your report2given enough time, books, slides, and internet bandwidth. A more in-depth mathematical part is expected in the next stage.In case adapting the algorithm to your problem requires some further mathematical derivation (example: developingan exact line search for your problem, when possible, or adapting an algorithm to deal more efficiently with the special structure of your problem), you are supposed to discuss it here with all the necessary mathematical detail. You are advised to send us a version of this section by e-mail as soon as it is done, so that we can catch misunderstandings as soon as possible and minimize the amount of work wasted. Note that we do not want to see code at this point: that would be premature to produce (for you) and unnecessarily complicated to read (for us).