\section{Method}

\subsection{Momentum descent approach}
The momentum descent approach add to the stochastic gradient descent a technique for accelerating gradient descent that accumulates a velocity vector in directions of persistent reduction in the objective across iterations. 
\\
This algorithm is a gradient-based optimization. The main idea of this type of algorithm is to minimize a loss function (or cost/error function) following the direction given by the gradient computed in the current point of the function.
\\
What can be a problem in the gradient descent is the large amount of examples that must be computed for each iteration. The opposite happens in the stochastic gradient descent where there is online training. However, the SGD is not as stable as the classical GD, but it requires only the evaluation of one example for each iteration. 
Form now on, when we use to talk about SDG, we refer to SDG with a subset of the training dataset, called mini-batch or batch SDG.
\\
Convergence of this algorithm is strictly connected to the choice of $\eta$, the learning rate parameter. It is important to gradually decrease $\eta$ over iterations, in order to guarantee convergence. A sufficient condition to ensure convergence of momentum descent is that 
\begin{equation}
\label{assumption:momentum_descent}
\sum_k \eta_k=\infty \  \text{ and } \ \sum_k \eta_k^2 < \infty
\end{equation}

The convergence rate is of $O(\frac{1}{\sqrt{k}})$ (after k steps) when the algorithm is applied to a convex problem. 
Moreover, if we assume that the Hessian matrix $\nabla^2f(x^*)$ is positive definite, means that the loss function is a strongly convex function and the convergence rate is $O(\frac{1}{k})$. \cite{goodfellow}
\\
Momentum descent approach uses the method of momentum to accelerate learning. It introduces a variable \textbf{v} in that we take account of the speed and direction at which the parameters move through parameter space. The role of momentum is determined by the hyperparameter $\alpha\in[0,1)$.
The update rule is the following:
\begin{equation}
\label{classical_momentum}
\textbf{v}_k = \alpha\textbf{v}_{k-1} + \eta\nabla\textit{L}(\textbf{W}_k).
\end{equation}
\begin{equation}
\label{update_momentum}
\textbf{W}_k = \textbf{W}_{k-1}  + \textbf{v}_k.
\end{equation}
The Nesterov momentum is a variant of the classical Momentum, the main idea is to add a correction factor to the Nesterov momentum,  for this reason they differ in the place where the gradient is evaluated. With Nesterov momentum, the rate of convergence goes from $O(\frac{1}{k})$ to $O(\frac{1}{k^2})$.
The update rule for the velocity vector \textbf{v} is the following:
\begin{equation}
\label{nesterov_momentum}
\textbf{v}_k = \alpha\textbf{v}_{k-1} + \eta\nabla\textit{L}(\textbf{W}_k + \alpha\textbf{v}_{k-1}).
\end{equation}
\subsubsection{Algorithm}
\begin{algorithm}[H]
	\caption{Stochastic Gradient Descent Algorithm. The learning rate $\eta$, the $\alpha$ term and the maximum number of iterations are given.}
	\label{alg:sgd}
	\begin{algorithmic}[1]
		\Require{Learning rate $\eta$ and momentum parameter $\alpha$}
		\Require{Maximum number of iteration and error threshold}
		\Procedure{Stochastic Gradient Descent}{}
		\State Initialize \textbf{W} and \textbf{v}
		\State $k \gets 0$
		\While {$k < max\_iterations$ \&\& $error\_th<e$}
		\State Sample a mini-batch of \textit{m} training examples \{\textit{$(x_0,y_0),(x_1,y_1),...,(x_m,y_m)$}\}
		\If {Nesterov Momentum}
		\State $\tilde{\textbf{W}} \gets \textbf{W} + \alpha \textbf{v}$
		\EndIf
		\State Compute gradient estimate: $\textbf{g} \gets \frac {1}{m} \nabla \sum_i\textit{L}(\tilde{\textbf{W}})$
		\State Compute velocity update: $\textbf{v} \gets \alpha \textbf{v} - \eta \textbf{g}$
		\State Apply update: $\textbf{W} \gets \textbf{W} + \textbf{v}$
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsection{Limited-memory quasi-Newton methods}
Limited-memory quasi-Newton methods are useful for solving large problems whose Hessian matrices cannot be computed at a reasonable cost or are not sparse.  
These methods maintain simple and compact approximations of Hessian matrices: instead of storing fully dense \textit{n} $\times$ \textit{n} approximations, they save only a few vectors of length \textit{n} that represent an approximation of it. 
 
The main idea of the L-BFGS algorithm is to use curvature information from only the most recent iterations to construct the Hessian approximation. Curvature information from earlier iterations, which is less likely to be relevant to the actual behavior of the Hessian at the current iteration is discarded in the interest of saving storage. 

\subsubsection{Limited memory BFGS}
L-BGFS algorithm derives from the BFGS method. So we begin our description of the L-BGFS method by recalling BFGS. 
Each step of the BGFS method has the form
\begin{equation}
\label{stepBFGS}
x_{k+1}=x_{k}-\alpha_{k}H_{k}\nabla f_{k} ,
\end{equation}
where $\alpha_{k}$ is the step length and $H_{k}$
is updated at every iteration by means of the formula

\begin{equation}
H_{k+1} = V^T_{k}H_{k}V_{k}+ \rho_{k}s_{k}s^T_{k}
\end{equation}
where 
\begin{align}
\rho_{k}=\frac{1}{y^T_{k}s_{k}}, & \qquad V_{k}=I-\rho_{k}y_{k}s^T_{k}, 
\end{align} 
and 
\begin{align}
s_{k}=x_{k+1}-x_{k}, & \qquad
y_{k}=\nabla f_{k+1} - \nabla f_{k}.
\end{align}
Since the inverse Hessian approximation $H_{k}$ will generally be dense, the cost of storing and manipulating it is prohibitive when the number of variable is large. To handle this problem in L-BFGS, we store a \textit{modified} version of $H_{k}$, by storing a certain number (say, \textit{m}) of the vector pairs $\{s_{i},y_{i}\}$ used in the formulas (5) and (6). The product $H_{k}\nabla f_{k}$ can be obtained by performing a sequence of inner products and vector summations involving $\nabla f_{k}$ and the pairs $\{s_{i},y_{i}\}$. After the new iterate is computed, the oldest vector pair in the set of pairs $\{s_{i},y_{i}\}$ is replaced by the new pair $\{s_{k},y_{k}\}$ obtained from the current step (7).
In this way, the set of vector pairs includes curvature information from the \textit{m} most recent iterations. Usually \textit{m} between 3 and 20 is chosen. 

We now describe the updating processes. An iteration \textit{k}, the current iterate $x_{k}$ and the set of vector pairs is given by $\{s_{i},y_{i}\}$ for $i=k-m, ..., k-1$. We first provide a sparse symmetric and positive definite Hessian approximation $H^0_{k}$ which approximates the inverse Hessian of $f$ 
%in contrast to the standard BFGS iteration, 
(this initial approximation is allowed to vary from iteration to iteration) and find by repeated application of the formula (5) that the L-BFGS approximation $H_{k}$ satisfies the following formula: 
\begin{equation} 
\begin{aligned}
&H_{k}=(V^T_{k-1}\cdots V^T_{k-m})H^0_{k}(V_{k-m}\cdots V_{k-1})\\           
&+\rho_{k-m}(V^T_{k-1}\cdots V^T_{k-m+1})s_{k-m}s^T_{k-m}(V_{k-m+1}\cdots V_{k-1})\\
&+\rho_{k-m+1}(V^T_{k-1}\cdots V^T_{k-m+2})s_{k-m+1}s^T_{k-m+1}(V_{k-m+2}\cdots V_{k-1})\\
&+ \cdots \\
&+\rho_{k-1}s_{k-1}s^T_{k-1}.  
\end{aligned}
\end{equation}
From this expression and given the number $m$ of BFGS corrections that are store we can derive a recursive procedure to compute the product $H_{k}\nabla f_{k}$ efficiently.

\begin{algorithm}[H]
	\caption{L-BFGS two loop recursion}
	\label{}
	\begin{algorithmic}[3]
	
		\State $q \gets \nabla f_{k}$;
		\For{$i=k-1,k-2,\cdots ,k-m$}
        \State $ \alpha_{i} \gets \rho_{i}s^T_{i}q$;
        \State $q \gets q - \alpha_{i}y_{i}$; 
      \EndFor
 	  \State $r \gets H^0_{k}q$;
 	  \For{$i=k-m,k-m+1,\cdots, k-1$}
 	  \State $\beta \gets \rho_{i}y^T_{i}r$; 
 	  \State $r \gets r + s_{i}(\alpha_{i}-\beta)$
 	  \EndFor
	  \State \textbf{stop} with result $H_{k}\nabla f_{k}=r$
	\end{algorithmic}
\end{algorithm}
Without considering the multiplication $H^0_{k}q $, the two-loop recursion scheme requires $4mn$ multiplications; if $H^0_{k}$ is diagonal, then $n$ additional multiplications are needed. Apart from being inexpensive, this recursion has the advantage that the multiplication by the initial matrix $H^0_{k}$ is isolated from the rest of the computations, allowing this matrix to be chosen freely and to vary between iterations. We may even use an implicit choice of $H^0_{k}$ by defining some initial approximation $B^0_{k}$ to the Hessian (not its inverse) and obtaining $r$ by solving the system $B^0_{k}r = q$. 

A method for choosing $H^0_{k}$ that has proved effective in practice is to set $H^0_{k}=\gamma_{k}I$  where 
\begin{equation}
\gamma_{k}= \frac{s^T_{k-1}y_{k-1}}{y^T_{k-1}y_{k-1}}
\end{equation}
$\gamma_{k}$ is the scaling factor that attempts to estimate the size of the true Hessian matrix along the most recent search direction. This choice helps to ensure that the search direction $p_{k}$ is well scaled, and as result the step length $\alpha_{k}=1$ is accepted in most iterations. To keep update stable of BFGS algorithm, it is important that line search be based on the Wolfe conditions
\begin{equation}
f(x_k + \alpha_kp_k ) \leq f (x_k ) + c_1 \alpha_k \nabla f_k^T p_k,
\end{equation}
\begin{equation}
\nabla f(x_k + \alpha_{k} p_k )^T p_k \geq c_2 \nabla f_k^T p_k ,
\end{equation}
or strong Wolfe conditions
\begin{equation}
f(x_k + \alpha_kp_k ) \leq f (x_k ) + c_1 \alpha_k \nabla f_k^T p_k,
\end{equation}
\begin{equation}
|\nabla f(x_k + \alpha_{k} p_k )^T p_k| \leq c_2 |\nabla f_k^T p_k| ,
\end{equation}
with 0<$c_1$<$c_2$<1.

\subsubsection{Algorithm}
Limited-memory variants  of  the  quasi-Newton  approach use  Hessian  approximations  that  can  be stored compactly by using just a few vectors of length \textit{n}. These methods are fairly robust, inexpensive, and easy to implement, but they do not converge rapidly.

\begin{algorithm}[H]
	\caption{L-BFGS.}
	\label{alg:LBFGS}
	\begin{algorithmic}[4]
		\Require{Starting point $x_0$;}
		\Require{Integer m>0;}
		\Require{Maximum number of iteration and error threshold;}
		\Procedure{LBFGS}{}
		\State Compute $k \gets 0$;
		\Repeat
		\State Choose \textbf{$H^0_k$};
		\State Compute $p_k \gets H_k\nabla \textit{f}_k$;
		\State Compute $x_{k+1} \gets x_{k}+ \alpha_{k}p_{k}$ where $\alpha_{k}$ is chosen to satisfy the Wolfe condition;
		\If {k>m}
		\State Discard the vector pair \{$s_{k-m},y_{k-m}$\} from storage;
		\EndIf
		\State Compute and save $s_k \gets x_{k+1}-x_{k}, y_k=\nabla \textit{f}_{k+1}-\textit{f}_k$;
		\State $k \gets k+1$
		\Until{\textbf{convergence.}}
		\EndProcedure 
	\end{algorithmic}
\end{algorithm}

Each iteration can be performed at cost of $O(m*n)$ arithmetic operation, where m is the number of steps stored in memory by parameter declaration.

\subsubsection{Algorithm Convergence}

L-BFGS guarantee global convergence under specific assumptions:
\begin{itemize} \label{convergence:assumption}
	\item The objective function f is twice continuously differentiable;
	\item The level set $ \mathcal{L} = \{x \in R^n | \mathit{f(x)}  \leq  \mathit{f(x_0)}\}$ is convex, and there exist positive constants m and M such that 
	\begin{align}
	m||z||^2 \leq z^T \nabla^2f(x)z \leq M||z||^2
	\end{align}
	for all z $\in R^n$  and x $\in \mathcal{L}$ .
\end{itemize}

The second point of this assumption implies that $\nabla^2f(x)$ is positive definite on $\mathcal{L}$ and that $\mathit{f}$ has a unique minimizer $x^*$ in $\mathcal{L}$.
\\
We can prove that, let $B_0$ be any symmetric positive definite initial matrix, and let $x_0$ be a starting point for which previous assumptions are satisfied. Then the sequence \{$x_k$\} generated by Algorithm \ref{alg:LBFGS} (with $\epsilon$ = 0) converges to the minimizer $x^*$ of $\mathit{f}$.
\\
Moreover, we can say that in this case, the rate of convergence of the iterates
is linear.






%\texttt{ Is the algorithm(if it is iterative) guaranteed to converge? Is it going to be stable and return a good approximation of the solution(if it is direct)?  Are there any relevant convergence results?}
 
  %\texttt{Are the hypotheses of theseconvergence results (convexity, compactness, differentiability, etc.) satisfied by your problem? If not, what are the“closest” possible results you have available, and why exactly they arenotapplicable?  Do you expect this to berelevant in practice?}

\subsection{Bundle methods}


Next, we expect a brief recall of the algorithmic properties that you expect to see in the experiments. Is the algorithm(if it is iterative) guaranteed to converge? Is it going to be stable and return a good approximation of the solution(if it is direct)? What is its complexity? Are there any relevant convergence results? Are the hypotheses of theseconvergence results (convexity, compactness, differentiability, etc.) satisfied by your problem? If not, what are the“closest” possible results you have available, and why exactly they arenotapplicable?  Do you expect this to berelevant in practice?Again, you are advised to send us a version of this section by e-mail as soon as it is done. Again, we do not wantto see code at this point.