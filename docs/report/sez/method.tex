\section{Method}

\subsection{Momentum descent approach}
The momentum descent approach add to the deterministic gradient descent a technique for accelerating gradient descent that accumulates a velocity vector in directions of persistent reduction in the objective across iterations. 
\\
This algorithm is a gradient-based optimization. The main idea of this type of algorithm is to minimize a loss function (or cost/error function) following the direction given by the gradient computed in the current point of the function.
\\
What can be a problem in the gradient descent is the large amount of example that must be computed for each iteration. The opposite happens in the stochastic gradient descent where there is an online training. However, the SGD is not as stable as the classical GD, but it requires only the evaluation of one example for each iteration. 
Form now on, when we talk about SGD, we refers to SGD with a subset of training dataset, called mini-batch or batch SGD.

Momentum descent approach use momentum to accelerate learning. It introduce a variable \textbf{v} that take account of the speed and direction at which the parameters move through parameter space. The role of momentum is determined by the hyperparameter $\alpha\in[0,1)$.
The update rule is the following:
\begin{equation}
\label{classical_momentum}
\textbf{v}_k = \alpha\textbf{v}_{k-1} + \eta\nabla\textit{L}(\textbf{W}_k).
\end{equation}
\begin{equation}
\label{update_momentum}
\textbf{W}_k = \textbf{W}_{k-1}  + \textbf{v}_k.
\end{equation}
The Nesterov momentum is a variant of the classical Momentum, the main idea is to add a correction factor at the classical momentum,  for this reason they differs in place where the gradient is evaluated. With Nesterov momentum the rate of convergence goes from $O(\frac{1}{k})$ to $O(\frac{1}{k^2})$.
The update rule for the velocity vector \textbf{v} is the following:
\begin{equation}
\label{nesterov_momentum}
\textbf{v}_k = \alpha\textbf{v}_{k-1} + \eta\nabla\textit{L}(\textbf{W}_k + \alpha\textbf{v}_{k-1}).
\end{equation}

\subsubsection{Algorithm}
\begin{algorithm}[H]
	\caption{Momentum Descent Algorithm. The learning rate $\eta$, the $\alpha$ term and the maximum number of iterations are given.}
	\label{alg:sgd}
	\begin{algorithmic}[1]
		\Require{Learning rate $\eta$ and momentum parameter $\alpha$}
		\Require{Maximum number of iteration and error threshold}
		\Procedure{Momentum Descent}{}
		\State Initialize \textbf{W} and \textbf{v}
		\State $k \gets 0$
		\While {$k < max\_iterations$ \&\& $error\_th<e$}
		\If {Nesterov Momentum}
		\State $\tilde{\textbf{W}} \gets \textbf{W} + \alpha \textbf{v}$
		\EndIf
		\State Compute gradient estimate: $\textbf{g} \gets \frac {1}{n} \nabla \sum_i\textit{L}(\tilde{\textbf{W}})$
		\State Compute velocity update: $\textbf{v} \gets \alpha \textbf{v} - \eta \textbf{g}$
		\State Apply update: $\textbf{W} \gets \textbf{W} + \textbf{v}$
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection{Method convergence}
Convergence of this algorithm is strictly connected to the choice of $\eta$, the learning rate parameter. It is important to gradually decrease $\eta$ over iterations, in order to guarantee convergence. A sufficient condition to ensure convergence of momentum descent is that 
\begin{equation}
\label{assumption:momentum_descent}
\sum_k \eta_k=\infty \  \text{ and } \ \sum_k \eta_k^2 < \infty
\end{equation}
\\
The convergence rate is of $O(\frac{1}{\sqrt{k}})$ (after k steps) when the algorithm is applied to a convex problem. 
Moreover, if we assume that the Hessian matrix $\nabla^2f(x^*)$ is positive definite, means that the loss function is a strongly convex function and the convergence rate is $O(\frac{1}{k})$. \cite{goodfellow}

Our objective function is not convex, so we need more constraint in order to use what we said before.
D.P. Bertsekas proved the following, which appears as proposition 1.2.4 of \cite{bert03}.
\\ Let \{$x_k$\} be a sequence generated by a gradient method $x_{k+1}=x_k+\alpha_kd_k$. Assume that for some constant L>0. we have
\begin{equation}
\label{assumption:momentum_descent2}
\| \nabla f(x)-\nabla f(y)\|\leq L\|x-y\|, \forall x,y \in R^n
\end{equation}
and that there exist positive scalars $c_1$, $c_2$ such that for all $k$  we have respect the Armijo-Wolfe conditions.
Suppose also that
\begin{equation}
\label{assumption:momentum_descent4}
\eta_k \to 0 \  \text{ , } \ \sum_k \eta_k = \infty
\end{equation}
The either $f(x^k)\to - \infty$ or else \{$f(x_k)$\} converges to a finite value and $\nabla f(x_k) \to 0$. Furthermore, every limit point of \{$x_k$\} is a stationary point of $f$.
The first fundamentals observation is that the loss function that we choose is a quadratic loss function and it is two times Lipschitz continue and derivable (as we said in \ref{LF:Properties}).

\subsection{Limited-memory quasi-Newton methods}
Limited-memory Quasi-Newton methods are useful for solving large unconstrained optimization problems in which Hessian matrices cannot be computed at a reasonable cost. More formally to minimize a function using the second order information, we create a quadratic model that approximate the function at the current $x_{k}$:
\begin{equation}
m_{k}(p)=L_{k}(p)+ \frac{1}{2}(p-k)^T\nabla^2 f(k)(p-k).
\end{equation}
where $L_{k}(p)$ is the first order model and the remaining is the second order term. Then we minimize the function using the information given by the model $m_{k}$.

Theoretically  the Hessian matrix $\nabla^2 f(k)$ need to be recomputed at every iteration. Instead in the Quasi-Netwon method the Hessian is substitute by an approximation $B_{k} \textrm{ (or } H_{k})$ to reduce the cost. Moreover in the BFGS we update $B_{k}$ using the curvature measured at previously step to avoid using $O(n^2)$ storage. The information about the approximated Hessian is stored inside vectors of length $n$. 

The main idea of the L-BFGS algorithm is to use curvature information from the $m$ recent iterations to construct the Hessian approximation, earlier iterations parameters are discarded in the interest of saving storage. 

\subsubsection{Limited memory BFGS}
\label{L-BFGS}
In the Broyden–Fletcher–Goldfarb–Shanno algorithm (BFGS) each step use the information given by the gradient $\nabla f_{k}$ and the approximated Hessian $H_{k}\textrm{ (or }B_{k})$ to minimize  a function updating its parameters $x_{k}$. Each step has the form: 
\begin{equation}
x_{k+1} = x_{k} - \alpha_{k}H_{k}\nabla f_{k}
\end{equation} 
where $\alpha_{k}$ is the step length.

In L-BFGS method to handle the complexity of storing an approximation of the Hessian, we save a certain number of vector $m$ composed of 
\begin{align}
s_{k}=x_{k+1}-x_{k}, & \qquad
y_{k}=\nabla f_{k+1} - \nabla f_{k}.
\end{align}
We can now approximate the product $H_{k}\nabla f_{k}$ by doing multiple products and vector summation using the $m$ vectors $\{s_{i},y_{i}\}$. At step $k$ after we compute the new iterate and the oldest vector $\{s_{m},y_{m}\}$ is replaced with the new one.

We first choose some initial Hessian approximation $H^0_{k}$ that is allowed to vary from iteration to iteration and then we find $H_{k}$ with

\begin{equation} 
\begin{aligned}
&H_{k}=(V^T_{k-1}\cdots V^T_{k-m})H^0_{k}(V_{k-m}\cdots V_{k-1})\\           
&+\rho_{k-m}(V^T_{k-1}\cdots V^T_{k-m+1})s_{k-m}s^T_{k-m}(V_{k-m+1}\cdots V_{k-1})\\
&+\rho_{k-m+1}(V^T_{k-1}\cdots V^T_{k-m+2})s_{k-m+1}s^T_{k-m+1}(V_{k-m+2}\cdots V_{k-1})\\
&+ \cdots \\
&+\rho_{k-1}s_{k-1}s^T_{k-1}.  
\end{aligned}
\end{equation}

where
\begin{align}
\rho_{k}= \frac{1}{y^T_{k}s_{k}}, & \qquad
V_{k} = I - \rho_{k} y_{k}s^T_{k},
\end{align}
formulas taken from \cite{numerical}.

Now we can derive an algorithm to compute the product $H_{k}\nabla f_{k}$

\begin{algorithm}[H]
	\caption{L-BFGS two loop recursion}
	\label{}
	\begin{algorithmic}[3]
	
		\State $q \gets \nabla f_{k}$;
		\For{$i=k-1,k-2,\cdots ,k-m$}
        \State $ \alpha_{i} \gets \rho_{i}s^T_{i}q$;
        \State $q \gets q - \alpha_{i}y_{i}$; 
      \EndFor
 	  \State $r \gets H^0_{k}q$;
 	  \For{$i=k-m,k-m+1,\cdots, k-1$}
 	  \State $\beta \gets \rho_{i}y^T_{i}r$; 
 	  \State $r \gets r + s_{i}(\alpha_{i}-\beta)$
 	  \EndFor
	  \State \textbf{stop} with result $H_{k}\nabla f_{k}=r$
	\end{algorithmic}
\end{algorithm}


A method for choosing $H^0_{k}$ that has proved effective in practice is to set $H^0_{k}=\gamma_{k}I$  where 
\begin{equation}
\gamma_{k}= \frac{s^T_{k-1}y_{k-1}}{y^T_{k-1}y_{k-1}}
\end{equation}
$\gamma_{k}$ is the scaling factor that attempts to estimate the size of the true Hessian matrix along the most recent search direction \cite{numerical}. This choice helps to ensure that the search direction $p_{k}$ is well scaled, and as result the step length $\alpha_{k}=1$ is accepted in most iterations. To keep update stable of BFGS algorithm, it is important that line search be based on the Wolfe conditions
\begin{equation}
f(x_k + \alpha_kp_k ) \leq f (x_k ) + c_1 \alpha_k \nabla f_k^T p_k,
\end{equation}
\begin{equation}
\nabla f(x_k + \alpha_{k} p_k )^T p_k \geq c_2 \nabla f_k^T p_k ,
\end{equation}
or strong Wolfe conditions
\begin{equation}
f(x_k + \alpha_kp_k ) \leq f (x_k ) + c_1 \alpha_k \nabla f_k^T p_k,
\end{equation}
\begin{equation}
|\nabla f(x_k + \alpha_{k} p_k )^T p_k| \leq c_2 |\nabla f_k^T p_k| ,
\end{equation}
with 0<$c_1$<$c_2$<1.

\subsubsection{Algorithm}
Limited-memory variants  of  the  quasi-Newton  approach use  Hessian  approximations  that  can  be stored compactly by using just few vectors of length \textit{n}. These methods are fairly robust, inexpensive, and easy to implement, but they do not converge rapidly.

\begin{algorithm}[H]
	\caption{L-BFGS.}
	\label{alg:LBFGS}
	\begin{algorithmic}[4]
		\Require{Starting point $x_0$;}
		\Require{Integer m>0;}
		\Require{Maximum number of iteration and error threshold;}
		\Procedure{LBFGS}{}
		\State Compute $k \gets 0$;
		\Repeat
		\State Choose \textbf{$H^0_k$};
		\State Compute $p_k \gets H_k\nabla \textit{f}_k$;
		\State Compute $x_{k+1} \gets x_{k}+ \alpha_{k}p_{k}$ where $\alpha_{k}$ is chosen to satisfy the Wolfe condition;
		\If {k>m}
		\State Discard the vector pair \{$s_{k-m},y_{k-m}$\} from storage;
		\EndIf
		\State Compute and save $s_k \gets x_{k+1}-x_{k}, y_k=\nabla \textit{f}_{k+1}-\textit{f}_k$;
		\State $k \gets k+1$
		\Until{\textbf{convergence.}}
		\EndProcedure 
	\end{algorithmic}
\end{algorithm}

Each iteration can be performed at cost of $O(mn)$ arithmetic operations, where $m$ is the number of steps stored in memory.

\subsubsection{Algorithm Convergence}

L-BFGS guarantee global convergence under specific assumption:
\begin{enumerate} \label{convergence:assumption}
	\item The objective function f is twice continuously differentiable;
	\item The level set $ \mathcal{L} = \{x \in R^n | \mathit{f(x)}  \leq  \mathit{f(x_0)}\}$ is convex, and there exist positive constants m and M such that 
	\begin{align}
	m||z||^2 \leq z^T \nabla^2f(x)z \leq M||z||^2
	\end{align}
	for all z $\in R^n$  and x $\in \mathcal{L}$ .
\end{enumerate}

The second point of this assumption implies that $\nabla^2f(x)$ is positive definite on $\mathcal{L}$ and that $\mathit{f}$ has a unique minimizer $x^*$ in $\mathcal{L}$.

Jorge Nocedal proved the following, which appears as theorem 6.1 of \cite{numerical}.
\\
\textbf{Th}: Let $B_0$ be any symmetric positive definite initial matrix, and let $x_0$ be a starting point for which previous assumption is satisfied. Then the sequence \{$x_k$\} generated by Algorithm \ref{alg:LBFGS} (with $\epsilon$ = 0) converges to the minimizer $x^*$ of $\mathit{f}$.
\\
Assumption 1 is satisfied, since our loss function f is twice continuously differentiable as we said in \ref{LF:Properties}. Assumption 2 is also satisfied for properties that we described in section \ref{LF:Properties}.
\\
Moreover, we can say that in this case the convergence rate of the iterates is linear. This rate is based on the size of $m$ introduced in section \ref{L-BFGS}. The more $m$ increases the more the convergence is quadratic as in the Newton method (quadratic convergence), on the contrary a decreasing of $m$ brings to a convergence which is similar to the convergence of the \textit{Gradient method} (linear convergence).







%\texttt{ Is the algorithm(if it is iterative) guaranteed to converge? Is it going to be stable and return a good approximation of the solution(if it is direct)?  Are there any relevant convergence results?}
 
  %\texttt{Are the hypotheses of theseconvergence results (convexity, compactness, differentiability, etc.) satisfied by your problem? If not, what are the“closest” possible results you have available, and why exactly they arenotapplicable?  Do you expect this to berelevant in practice?}

\subsection{Bundle methods}


Next, we expect a brief recall of the algorithmic properties that you expect to see in the experiments. Is the algorithm(if it is iterative) guaranteed to converge? Is it going to be stable and return a good approximation of the solution(if it is direct)? What is its complexity? Are there any relevant convergence results? Are the hypotheses of theseconvergence results (convexity, compactness, differentiability, etc.) satisfied by your problem? If not, what are the“closest” possible results you have available, and why exactly they arenotapplicable?  Do you expect this to berelevant in practice?Again, you are advised to send us a version of this section by e-mail as soon as it is done. Again, we do not wantto see code at this point.