\section{Method}

\subsection{Momentum descent approach}
\label{MDA}
The momentum descent approach add to the deterministic gradient descent (GD) a velocity vector in directions of persistent reduction in the objective function across iterations for accelerating gradient descent.
\\
This algorithm is a gradient-based optimization. The main idea of this type of algorithm is to minimize a loss function (or cost/error function) following the direction given by the gradient computed in the current point of the function.
\\
What can be a problem in the gradient descent is the large number of examples that must be computed at each iteration. Instead, in the stochastic gradient descent (SGD) a small number of examples  are computed in online training. However, the SGD is not as stable as the classical GD, but it requires only the evaluation of one example for each iteration. 
Form now on, when we talk about SGD, we refer to SGD with a subset of the training dataset, called mini-batch or batch SGD.

Momentum descent approach use momentum to accelerate learning. It introduces a variable \textbf{v} that take account of the speed and the direction at which the parameters move through parameters space. The intensity of momentum is determined by the hyperparameter $\alpha\in[0,1)$.
The update rule is the following:
\begin{equation}
\label{classical_momentum}
\textbf{v}_k = \alpha\textbf{v}_{k-1} + \eta\nabla\textit{L}(\textbf{W}_k).
\end{equation}
\begin{equation}
\label{update_momentum}
\textbf{W}_k = \textbf{W}_{k-1}  + \textbf{v}_k.
\end{equation}
We would like to observe that the method choose a descent direction equals to $\frac{\partial f}{\partial d_i}(x_i)<0$. This means that $\langle d_i, \nabla f(x_i)\rangle <0$ and the $\cos(\theta_i)>0$. The main idea is to take the information given by the directional derivatives on the way that our function decrease and follow these directions. So, the method follows a descent direction, the best case is to take the direction $d_i$ with the same direction of $-\nabla f(x_i)$.\\
The Nesterov momentum is a variant of the classical Momentum, the main idea is to add a correction factor at the classical momentum,  for this reason they differ in the place where the gradient is evaluated. With Nesterov momentum the rate of convergence goes from $O(\frac{1}{k})$ to $O(\frac{1}{k^2})$.
The update rule for the velocity vector \textbf{v} is the following:
\begin{equation}
\label{nesterov_momentum}
\textbf{v}_k = \alpha\textbf{v}_{k-1} + \eta\nabla\textit{L}(\textbf{W}_k + \alpha\textbf{v}_{k-1}).
\end{equation}

Also, there exists a variant of the Nesterov momentum that guarantees a monotonic sequence of objective values and it is named Nesterov Momentum Approach with restart. More information can be found in the paper \cite{NWR}. This property could be useful to state that the gradient is Lipschitz continuous, but we did not have implemented it.


\subsubsection{Algorithm}
\begin{algorithm}[H]
	\caption{Momentum Descent Algorithm. The learning rate $\eta$, the $\alpha$ term and the maximum number of iterations are given.}
	\label{alg:sgd}
	\begin{algorithmic}[1]
		\Require{Learning rate $\eta$ and momentum parameter $\alpha$}
		\Require{Maximum number of iteration and error threshold}
		\Procedure{Momentum Descent}{}
		\State Initialize \textbf{W} and \textbf{v}
		\State $k \gets 0$
		\While {$k < max\_iterations$ \&\& $error\_th<e$}
		\If {Nesterov Momentum}
		\State $\tilde{\textbf{W}} \gets \textbf{W} + \alpha \textbf{v}$
		\EndIf
		\State Compute gradient estimate: $\textbf{g} \gets \frac {1}{n} \nabla \sum_i\textit{L}(\tilde{\textbf{W}})$
		\State Compute velocity update: $\textbf{v} \gets \alpha \textbf{v} - \eta \textbf{g}$
		\State Apply update: $\textbf{W} \gets \textbf{W} + \textbf{v}$
		\EndWhile
		\EndProcedure
	\end{algorithmic}
\end{algorithm}

\subsubsection{Method convergence}
\label{MDA-convergence}
Convergence of this algorithm is strictly connected to the choice of $\eta$, the learning rate parameter. It is important to gradually decrease $\eta$ over iterations, to guarantee convergence. A sufficient condition to ensure convergence of momentum descent is that 
\begin{equation}
\label{assumption:momentum_descent}
\sum_k \eta_k=\infty \  \text{ and } \ \sum_k \eta_k^2 < \infty
\end{equation}
\\
The convergence rate is of $O(\frac{1}{\sqrt{k}})$ (after k steps) when the algorithm is applied to a convex problem. 
Moreover, if we assume that the Hessian matrix $\nabla^2f(x_*)$ is positive definite, means that the loss function is a strongly convex function and the convergence rate is $O(\frac{1}{k})$. \cite{goodfellow}

Our objective function is not convex, so we need more assumptions to ensure the convergence.
D.P. Bertsekas proved the following, which appears as proposition 1.2.4 of \cite{bert03}.
\\ Let \{$x_k$\} be a sequence generated by a gradient method (in our specific case a Momentum descent approach with a direction $d_{k}$ defined as stated in \S \ref{MDA}) $x_{k+1}=x_k+\alpha_kd_k$. Assume that for some constant L>0 we have
\begin{equation}
\label{assumption:momentum_descent2}
\| \nabla f(x)-\nabla f(y)\|\leq L\|x-y\|, \forall x,y \in R^n
\end{equation}
and that there exist positive scalars $c_1$, $c_2$ such that for all $k$ the Armijo-Wolfe conditions are respected.
Suppose also that
\begin{equation}
\label{assumption:momentum_descent4}
\eta_k \to 0 \  \text{ , } \ \sum_k \eta_k = \infty
\end{equation}
Then either $f(x_k)\to - \infty$ or else \{$f(x_k)$\} converges to a finite value and $\nabla f(x_k) \to 0$. Furthermore, every limit point of \{$x_k$\} is a stationary point of $f$.
The first fundamentals observation is that the loss function that we choose is a quadratic and it is two times Lipschitz continue and derivable (as we said in \S \ref{LF:Properties}).

As stated in \cite{numerical} in Chapter 3 the Armijo-Wolfe conditions are defined as:
\begin{equation}
\label{armijo}
f(x_k + \alpha_kp_k ) \leq f (x_k ) + c_1 \alpha_k \nabla f_k^T p_k,
\end{equation}
\begin{equation}
\label{wolfe}
\nabla f(x_k + \alpha_{k} p_k )^T p_k \geq c_2 \nabla f_k^T p_k ,
\end{equation}

Also, our implementation does not use a line search to decide the step length and this means that the Bertsekas results are not valid in our case.

Indeed, we have implemented a grid search for the choice of the fixed step length. Grid search is the traditional method of hyperparameters optimization. It simply makes a complete search over a given subset of the hyperparameters space of the training algorithm. In our case, we used as hyperparameters the step length and the momentum rate. Grid search suffers from high dimensional spaces, but in our case, the dimensional space generated from the two hyperparameters is not too high. In theory, you have to tests all possible permutation combinations of hyperparameters of given Machine Learning algorithm, in practice, grid search is used to create different size of hyperparameters permutation combinations that gradually became more fine-grained. 

We cannot state anything a priori regarding the convergence of the Nesterov Momentum Method applied to a non-convex problem with fixed step size found through the grid search method. But after analysing the results of the experimental phase, we can state that in our problems Nesterov Descent Approach has a linear convergence.

\subsection{Limited-memory quasi-Newton methods}
Limited-memory Quasi-Newton methods are useful for solving large unconstrained optimization problems in which Hessian matrices cannot be computed at a reasonable cost. More formally to minimize a function using the second order information, we create a quadratic model that approximate the function at the current $x_{k}$:
\begin{equation}
m_{k}(p)=L_{k}(p)+ \frac{1}{2}(p-k)^T\nabla^2 f(k)(p-k).
\end{equation}
where $L_{k}(p)$ is the first order model and the remaining is the second order term. Then we minimize the function using the information given by the model $m_{k}$ and the direction compute as 
\begin{equation}
d_{k} \leftarrow -[\nabla^2f(x_{k})]^{-1}\nabla f(x_{k}).
\end{equation}

Theoretically  the Hessian matrix $\nabla^2 f(k)$ needs to be recomputed at every iteration. Instead in the Quasi-Netwon method the Hessian is substituted by an approximation $ H_{k}$ to reduce the cost. 

The main idea of L-BFGS is to use the curvature information from the $m$ previously step to construct $B_{k}$, the inverse of the Hessian ($H_{k}^{-1}$), to avoid using $O(n^2)$ storage. The information about $B_{k}$ is stored inside vectors of length $n$. Earlier iterations parameters are discarded in the interest of saving storage.

\subsubsection{Limited memory BFGS}
\label{L-BFGS}
In the Broyden–Fletcher–Goldfarb–Shanno algorithm (BFGS) each step use the information given by the gradient $\nabla f_{k}$ and the approximated inverse of the Hessian $B_{k}$ to minimize  a function updating its parameters $x_{k}$. Each step has the form: 
\begin{equation}
\label{direction}
x_{k+1} = x_{k} - \alpha_{k}B_{k}\nabla f_{k}
\end{equation} 
where $\alpha_{k}$ is the step length.

In L-BFGS method to handle the complexity of storing an approximation of the Hessian, we save a certain number of vector $m$ composed of 
\begin{align}
s_{k}=x_{k+1}-x_{k}=\alpha_{k}d_{k}, & \qquad
y_{k}=\nabla f_{k+1} - \nabla f_{k}.
\end{align}
We can now approximate the product $B_{k}\nabla f_{k}$ by doing multiple products and vector summation using the $m$ vectors $\{s_{i},y_{i}\}$. At step $k$ after we compute the new iterate and the oldest vector $\{s_{m},y_{m}\}$ is replaced with the new one.

We first choose some initial Hessian approximation $B^0_{k}$ that is allowed to vary from iteration to iteration and then we find $B_{k}$ with

\begin{equation} 
\begin{aligned}
&B_{k}=(V^T_{k-1}\cdots V^T_{k-m})B^0_{k}(V_{k-m}\cdots V_{k-1})\\           
&+\rho_{k-m}(V^T_{k-1}\cdots V^T_{k-m+1})s_{k-m}s^T_{k-m}(V_{k-m+1}\cdots V_{k-1})\\
&+\rho_{k-m+1}(V^T_{k-1}\cdots V^T_{k-m+2})s_{k-m+1}s^T_{k-m+1}(V_{k-m+2}\cdots V_{k-1})\\
&+ \cdots \\
&+\rho_{k-1}s_{k-1}s^T_{k-1}.  
\end{aligned}
\end{equation}

where
\begin{align}
\rho_{k}= \frac{1}{y^T_{k}s_{k}}, & \qquad
V_{k} = I - \rho_{k} y_{k}s^T_{k},
\end{align}
formulas taken from \cite{numerical}.

Now we can derive an algorithm to compute the product $B_{k}\nabla f_{k}$

\begin{algorithm}[H]
	\caption{L-BFGS two loop recursion}
	\label{L-BFGSTwoLoop}
	\begin{algorithmic}[3]
	
		\State $q \gets \nabla f_{k}$;
		\For{$i=k-1,k-2,\cdots ,k-m$}
        \State $ \alpha_{i} \gets \rho_{i}s^T_{i}q$;
        \State $q \gets q - \alpha_{i}y_{i}$; 
      \EndFor
 	  \State $r \gets B^0_{k}q$;
 	  \For{$i=k-m,k-m+1,\cdots, k-1$}
 	  \State $\beta \gets \rho_{i}y^T_{i}r$; 
 	  \State $r \gets r + s_{i}(\alpha_{i}-\beta)$
 	  \EndFor
	  \State \textbf{stop} with result $B_{k}\nabla f_{k}=r$
	\end{algorithmic}
\end{algorithm}


A method for choosing $B^0_{k}$ that has been proved effective in practice is to set $B^0_{k}=\gamma_{k}I$  where 
\begin{equation}
\gamma_{k}= \frac{s^T_{k-1}y_{k-1}}{y^T_{k-1}y_{k-1}}
\end{equation}

$\gamma_{k}$ is the scaling factor that attempts to estimate the size of the true Hessian matrix along the most recent search direction \cite{numerical}. This choice helps to ensure that the search direction $p_{k}$ is well scaled.

To keep the update of the parameters stable using the approximated inverse of the Hessian $B_{k}$, a condition based on the knowledge gained during the latest step is imposed. The new model must respect old information 
\begin{equation}
\label{LBFGS-FirstCondition}
\nabla m_{k+1}(x_{k})=\nabla f(x_{k}),
\end{equation}
that is ensured if the \textit{secant equation}
\label{secant-equation}
\begin{equation}
B_{k+1}y_{k}=s_{k}
\end{equation}
is satisfied. This is possible if $s_{k}$ and $y_{k}$ satisfy the \textit{curvature condition} 
\begin{equation}
\label{eq:curvature-condition}
s_{k}^T y_{k}>0.
\end{equation}
The \textit{curvature condition} is satisfied when $f$ is strongly convex but for nonconvex functions we need to restrict the step $\alpha_{k}$ used in the line search procedure to guarantee the satisfiability of it.
Moreover the \textit{curvature condition} is satisfied in the line search if we impose Wolfe conditions \ref{armijo} and \ref{wolfe}
%\begin{equation}
%(x_k + \alpha_kp_k ) \leq f (x_k ) + c_1 \alpha_k \nabla %f_k^T p_k,
%\end{equation}
%\begin{equation}
%\nabla f(x_k + \alpha_{k} p_k )^T p_k \geq c_2 \nabla %f_k^T p_k ,
%\end{equation}
or strong Wolfe conditions
\begin{equation}
f(x_k + \alpha_kp_k ) \leq f (x_k ) + c_1 \alpha_k \nabla f_k^T p_k,
\end{equation}
\begin{equation}
|\nabla f(x_k + \alpha_{k} p_k )^T p_k| \leq c_2 |\nabla f_k^T p_k| ,
\end{equation}
with 0<$c_1$<$c_2$<1. 

As shown in \cite{numerical}, we need to ensure that among all the symmetric matrix satisfying the secant equation, $B_{k+1}$ is the closest to the current matrix $
B_{k}$. This condition of closeness to $B_{k}$ is resolved by solving the problem
\begin{equation}
B_{k+1} = \min_{B} \parallel B - B_{k} \parallel
\end{equation}
with 
\begin{equation}
B = B^T, \quad B{y_{k}}= s_{k}
\end{equation}
for $\parallel \cdot \parallel_{F} $ the formula to compute $B_{k+1}$ is found.


\subsubsection{Algorithm}
Limited-memory variants  of  the  quasi-Newton  approach use  Hessian  approximations  that  can  be stored compactly by using just few vectors of length \textit{n}. These methods are fairly robust, inexpensive and easy to implement.

\begin{algorithm}[H]
	\caption{L-BFGS.}
	\label{alg:LBFGS}
	\begin{algorithmic}[4]
		\Require{Starting point $x_0$;}
		\Require{Integer m>0;}
		\Require{Maximum number of iteration and error threshold;}
		\Procedure{LBFGS}{}
		\State Compute $k \gets 0$;
		\Repeat
		\State Choose \textbf{$B^0_k$};
		\State Compute $p_k \gets B_k\nabla \textit{f}_k$;
		\State Compute $x_{k+1} \gets x_{k}+ \alpha_{k}p_{k}$ where $\alpha_{k}$ is chosen to satisfy the Wolfe condition;
		\If {k>m}
		\State Discard the vector pair \{$s_{k-m},y_{k-m}$\} from storage;
		\EndIf
		\State Compute and save $s_k \gets x_{k+1}-x_{k}, y_k=\nabla \textit{f}_{k+1}-\textit{f}_k$;
		\State $k \gets k+1$
		\Until{\textbf{convergence.}}
		\EndProcedure 
	\end{algorithmic}
\end{algorithm}

Each iteration can be performed at cost of $O(mn)$ arithmetic operations, where $m$ is the number of steps stored in memory.

Our implementation uses the update rule of BFGS-Level described in \cite{PaperL-BFGS-NeuralNetwork} chapter 3. The main idea is to avoid the second-order interactions between weights of different levels and consider a separate approximate Hessian matrix (in our case defined as $B_{k}$) for each level of the network. As written in \cite{PaperL-BFGS-NeuralNetwork} this approach reduces considerably the total size of the matrix to be computed, since the Hessian matrix of a neural network is sparse with respect to the output layer because there are no interactions between output neurons.

\subsubsection{Algorithm Convergence}
\label{L-BFGSConv}
L-BFGS guarantee global convergence under specific assumptions:
\begin{enumerate} \label{convergence:assumption}
	\item The objective function f is twice continuously differentiable;
	\item The approximated inverse Hessian $B_{k}$ need to be positive definite to take the correct descent direction.
\end{enumerate}


Assumption 1 is satisfied, since our loss function f is twice continuously differentiable as we said in \ref{LF:Properties}. 
                                                       
Assumption 2 is satisfied because L-BFGS computes an approximation of the inverse of the Hessian $B_{k}$ that is directly multiplied with the gradient $\nabla f_{k}$ to obtain a direction $p_{k}$ (see algorithm \ref{L-BFGSTwoLoop}), so methods that manually modify the Hessian to make it positive definite are not used. Indeed, to produce a descent direction we need to guarantee that $B_{k+1}$  will be positive definite. This is true whenever $B_{k}$ (past approximation) is positive definite, as shown in \cite{numerical} section 6.1 (subsection "Properties of the BFGS method"). Produce $B_{k+1}$ that is correctly defined is ensured whenever the initial approximation $B_{0}$ (first $B_{k}$) is positive definite and the \textit{secant equation} is satisfied for every $k$ step of the minimization process (which is our case since $B_{0}$ it is initialized as positive definite and the secant equation is checked at every iteration). 

Also if the $\alpha_{k}$ (step length) is computed by an inexact line search that satisfies the Wolfe conditions and the first step tried has always value equal to 1, then the search direction of a quasi-Newton method approximates the Newton direction well enough so that the step size $\alpha_{k}$ will satisfy the Wolfe conditions as the iterates converge to the solution (as reported in \cite{numerical} chapter 3.3 paragraph "Quasi-Newton method"). This presents a condition that if it is satisfied produces a superlinearly convergent iteration for all $x_{k}$ in the proximity of a solution $x_{*}$ in a convex case. But in our case, since the function to be optimized is non-convex we need to make additional assumptions. We first present the assumptions needed to obtain a superlinear convergence in the convex case, after that the additional assumptions needed for the non-convex case are discussed.

\textit{Theorem 3.6} of \cite{numerical} states: suppose that $f: \textnormal{ }\mathbb{R}^n \rightarrow \mathbb{R}$ is twice continuously differentiable. Consider the iteration $x_{k+1} = x_{k} + \alpha_{k}d_{k}$, where $d_{k}$ is a descent direction and $\alpha_{k}$ satisfies the Wolfe conditions with $c_{1} <= \frac{1}{2}$ (which is our case). If the sequence $\{ x_{k} \}$ converges to a point $x_{*}$ such that $\nabla f(x_{*}) = 0$ and $\nabla^2f(x_{*})$ is positive definite, and if the search direction satisfies: 
\begin{equation}
\label{FirstConditionAlpha}
 \lim_{k\to\infty}\frac{\Vert (\nabla f_{k} + \nabla^2 f_{k} d_{k})\Vert}{\Vert d_{k} \Vert} = 0.
\end{equation} 
then 
\begin{itemize}
	\item the step length $\alpha_{k} = 1$ is admissible for all $k$ greater than a certain index $k_{0}$;
	\item if $\alpha_{k} = 1$ for all $k>k_{0}, x_{k}$ converges to $x_{*}$ superlinearly.
\end{itemize}

 Indeed since the objective function is twice continuously differentiable (as mention in \S \ref{LF:Properties}), $c_{1}<\frac{1}{2}$ (we use 1e-4) and $d_{k}$ is equal to $B_{k}\nabla f_{k}$ (which is our case as mention in \ref{direction}) \textit{Theorem 3.6} obtained equation \ref{FirstConditionAlpha} is equivalent to: 
\begin{equation}
\label{SecondConditionAlpha}
 \lim_{k\to\infty}\frac{\Vert (B_{k} - \nabla^2f(x_*) d_{k})\Vert}{\Vert d_{k} \Vert} = 0.
\end{equation}

This shows that a superlinear convergence rate can be obtained even if the sequence of $B_{k}$ does not converge to $\nabla^2f(x_*)$. It is sufficient that the $B_{k}$ is positive definite (which is our case as mention above, section 6.1 of \cite{numerical}) and its accuracy approximations increase to $\nabla^2f(x_*)$ progressively. The condition \ref{SecondConditionAlpha} is sufficient to ensure the local superlinear convergence of L-BFGS (more in general of the quasi-Newton methods). This is formalized in \textit{Theorem 3.7} (of \cite{numerical}):
suppose that $f: \textnormal{ }\mathbb{R}^n \rightarrow \mathbb{R}$ is twice continuously differentiable. Consider the iteration: $x_{k+1}=x_{k} + p_{k}$ (i.e. the step length $\alpha_{k}$ is uniformly 1) and that $p_{k}$ is given by $B_{k}\nabla f_{k}$ (as mention in \ref{direction}). Let us assume that $x_{k}$ converges to a point $x_{*}$ such that $\nabla f(x_{*}) = 0$ and $\nabla^2f(x_{*})$ is positive definite. Then $x_{k}$ converges superlinearly if and only if \ref{SecondConditionAlpha} holds. The proof can be found in \cite{numerical} (Theorem 3.7). 

In a non-convex case (which is our case as mentioned in \S \ref{LF:convexity}) additional assumptions are needed to guarantee a superlinear convergence. The sequence $|| x_{k} - x^* ||$ has to converge in a such a way that:
\begin{equation}
\label{eq:convergence-sequence}
\sum_{k=1}^{\infty}|| x_{k} - x^* ||<\infty
\end{equation}
is obtained. Then \textit{Assumption 6.2} of \cite{numerical} is required: the Hessian matrix G is Lipschitz continuous at $x^*$, that is: 

\begin{equation}
\label{eq:HessianLC}
|| G(x)-G(x^*)  || \le L || x - x^*||
\end{equation}

for all $x$ near $x^*$, where L is a positive constant. These two properties are needed by \textit{Theorem 6.6} of \cite{numerical}, it states:  suppose that $f$ is twice continuously differentiable and that the iterates generated by the BFGS algorithm converge to a minimizer $x^*$ at which equation \ref{eq:HessianLC} holds. Suppose also that  equation \ref{eq:convergence-sequence} holds, then $x_{k}$ converges to $x^*$ with a superlinear rate. However, in our case we cannot verify these conditions, because we cannot state that the equation \ref{eq:convergence-sequence} is satisfied. Supposing that it is satisfied, we have to state that equation \ref{eq:HessianLC} holds. To do this we refer to the proof of \textit{Assumption 6.2} of \cite{numerical}. For these reasons we cannot surely state that a superlinear convergence rate will be obtained. 

But, during the experimental phase we developed an Armijo-Wolfe line search that has a maximum number of iteration to obtain a feasible value in a finite interval of time. For this reason, we guarantee the property of the matrix $B_{k}$ to be positive definite by checking in each iteration the satisfiability of the \textit{curvature condition} and we found that the rate of convergence of the algorithm is superlinear (see fig. \ref{R-Monk1}) even in our non-convex case.

If $m$ is very small the behaviour of L-BFGS will be similar to the Gradient method and if $m$ is very large the behaviour is similar as BFGS. However, $m$ is problem dependent so a good trade-off between CPU computational time and convergence speed needs to be found when choosing it.

\subsection{Bundle methods}
\label{Bundle-methods}
Bundle methods are used mainly with non-differentiable functions. The main idea of these methods is to use past subgradients as global information. In convex function we know that at each iteration we get a subgradient in which the epigraph of the loss function is always above the epigraph of the subgradient, so epi($L_x$) $ \supseteq $ epi($f$) .\\ 
We define as \{$x_i$\} the sequence of iterates that will hopefully converge at some point $x_*$. 
The bundle of first-order information is:
\begin{equation}
\{x_{k}\}\rightarrow \mathcal{B}=\{(x_{k}, f^k=f(x_{k}), g^{k} \in \partial f(x_{k})\}
\end{equation}
and the cutting-plane model of $f$ - $(1 +\epsilon)$-order model as:
\begin{equation}
\label{CP-model}
f_B(x) = \max{\{f^k+ \langle g^k,x - x_l \rangle:  (x_k,f^k,g^k)\in B\}}
\end{equation}
in which $g^i$ is the subgradient chosen at $i^{th}$ iteration.
Before getting deep in defining the bundle methods, we need to introduce some information about subgradient.
\subsubsection{Subgradient}
The subgradient is a generalization of gradients appropriate for convex functions, including those which are not necessarily smooth.
The subgradient $s$ of $f(x)$ is defined as:
\begin{equation}
f(y) \geq f(x) +s(y-x) \quad \forall y \in \mathbb{R}^n.
\end{equation}
The set of all subgradients at a point is called the sub-differential, and it is denoted by $\partial f(x)$. 
If at a point the function is differentiable, the subgradient is exactly the gradient of the function at that point ($\partial f(x)= \{\nabla f(x) \}$).
In a point in which the function is not differentiable, \textit{s} is an infinity set of point closed and convex.
If we choose $s$ such that it respects:
\begin{equation}
\frac{\partial f}{\partial d}(x)=\sup\{\langle s,d \rangle : s \in \partial f(x)\} 
\end{equation} 
we have that $d$ is a descent direction $\iff \langle s,d\rangle <0 \quad \forall s \in \partial f(x)$.
\\ We defined $s_*$ as the $s$ with the steepest descent direction:
\begin{equation}
s_*= -argmin\{\parallel s\parallel : s\in \partial f(x) \}
\end{equation} 
\subsubsection{Master problem}
We know that $f_B$ is a polyhedral function, for this reason is composed of a finite number of linear functions.
In this way, we can define the master problem as:
\begin{equation}
\label{MP}
min\{f_B(x)\} = min\{v : v \geq f^k + \langle g^k, x-x_k \rangle \qquad (x_k, f^k, g^k) \in B \}
\end{equation} 
A method to use efficiently the equation \ref{MP} is the \textit{cutting plane algorithm} in which at every iteration we add the knowledge of the $g^i$ sub-gradient at $B$ if the $v^i$ is greater than $f(x_*)$ otherwise we find the optimal value of $f$.  
Unfortunately, the master problem as defined in \ref{MP} have some issue when $x_i$ is very far from $x_*$. Due to this, the cutting plane algorithm does not work very well in practice.
For this reason, we use a stabilized master problem: 
\begin{equation}
\label{SMP}
 d_{k}=argmin\{f_B(x)\ + \mu \parallel x- \overline{x}\parallel_{2}^2/2 \}
\end{equation} 
in which $\overline{x}$ is a particular point called \textit{stability center} and $\mu$ is the \textit{stability parameter}.  It is a term that it is introduced to take close the new point $x_{*B}$ from $\overline{x}$.
Usually, $\overline{x}$ is chosen as the best $x_i$ at the moment and $\mu$ is a hyperparameter.
For a large $\mu$ the parable is extremely sticky and small movement in the next iteration are done.
Instead,  for a small $\mu$, the parable is too flat and in this case the same result as in the un-stabilized cutting plane method can be obtained. For this reason, is important to find the right $\mu$ to use.
\subsubsection{Proximal Bundle Method}
The main idea behind the Proximal Bundle Method is to approximate the objective function $f$ by the \textit{cutting-plane} model as mention in \ref{CP-model}. To obtain the step size to apply into the result $d_{k}$ obtained solving the \textit{Master problem}, the following line search procedure is used (as described in \cite{NonsmoothOPT} \S12.1): 
Assume that $m_{L} \in (0, \frac{1}{2}, m_{R} \in (m_{L},1))$ \textnormal{ and } $ \bar{t} \in (0,1])$ are fixed line search parameters. The first search for the largest number $t_{L}^k \in [0,1]$ such that $t_{L}^k \geq \bar{t}$ and
\begin{equation}
\label{linesearchCondition}
f(x_{k}+t_{L}^k d_{k}) \leq f(x_{k}) + m_{L}t_{L}^k v_{k}, 
\end{equation}
where $v_{k}$ is the predicted amount of descent 
\begin{equation}
v_{k} = \hat{f_{k}}(x_{k}+d_{k}) - f(x_{k}) < 0.
\end{equation}
If exists that parameter a \textit{long serious step} is taken:
\begin{equation}
x_{k+1} = x_{k} + t_{L}^k d_{k} \textnormal{ and } y_{k+1}= x_{k+1}.
\end{equation}
Otherwise, if \ref{linesearchCondition} holds but $0 < t_{L}^k < \hat{t}$, a \textit{short serious step} is taken:
\begin{equation}
x_{k+1} = x_{k} + t_{L}^k d_{k} \textnormal{ and } y_{k+1}= x_{k}+t_{R}^k d_{k}
\end{equation} 
and, if $t_ {L}^k=0$, a \textit{null step} is taken: 
\begin{equation}
x_{k+1} = x_{k} + t_{L}^k d_{k} \textnormal{ and } y_{k+1}= x_{k}+t_{R}^k  d_{k}
\end{equation}
where $t_{R}^h > t_{L}^h$ is such that 
\begin{equation}
- \beta_{k+1}^{k+1}+ g^i d_{k} \geq m_{R}v_{k}.
\end{equation}
To use  $ \beta_{k+1}^{k+1}$, $\alpha_{j}$ that is a \textit{linearization error} has to be introduced: 
\begin{equation}
\alpha_{j}^k = f(x_{k}) - f(y_{j}) -g^k(x_{k}-\hat{x}).
\end{equation}
But in our case, since \textit{f} is non-convex, $\alpha_{j}^k$ can be replaced by $\beta_{j}^k$ called \textit{subgradient locality measure} (see chapter 12 of \cite{NonsmoothOPT}): 
\begin{equation}
\beta_{j}^k = max\{|\alpha_{j}^k|,\gamma || x_{k}-y_{j} ||^2\},
\end{equation}
\begin{algorithm}[H]
	\caption{PBM.}
	\label{alg:PBM}
	\begin{algorithmic}[4]
		\Procedure{PBM}{}
		\State initialization:
		\State $x_1 \in \mathbb{R}^n, \,  J=\{1\}, \, \overline{t} \in (0,1], \, m_{L} \in (0,\frac{1}{2}), \, u_1>0, \, \epsilon>0 \textnormal{ and } v_0= -\epsilon$;
		\State Set k=1;
		\State Evaluate f($x_1$) and $ \xi_1 \in \partial f (x_1)$;
		\Repeat
		\State $d_k\leftarrow argmin_{d\in \mathbb{R}^n }\{f_B(x_{k} + d) +\frac{1}{2}\mu_{k}d^{T}d\}$;
		\State Compute $v_{k} \leftarrow f_B(x_{k} + d) - f(x_k)$;
		\State Find step sizes $t^{k}_{L}$ and  $t^{k}_{R}$;
		\If{$f(x_{k} + t^{k}_{L} *d_{k} )\leq f(x_{k}) + m_{L}*t^{k}_{L}*v_{k}$}
			\If{$t^{k}_{L}> \overline{t}$}
				\State LONG SERIOUS STEP
				\State Set $x_{k+1}= x_{k} + t^{k}_{L}*d_{k}$;
				\State Set $y_{k+1}= x_{k+1}$
				\State Evaluate $\xi_{k+1} \in \partial f (y_{k+1})$ 
				\State END LONG SERIOUS STEP
			\Else
				\State SHORT SERIOUS STEP
				\State Set $x_{k+1}= x_{k} + t^{k}_{L}*d_{k}$;
				\State Set $y_{k+1}= x_{k}+t^{k}_{R}*d_{k}$
				\State Evaluate $\xi_{k+1} \in \partial f (y_{k+1})$ 
				\State END SHORT SERIOUS STEP
			\EndIf
		\Else
			\State NULL STEP
			\State Set $x_{k+1}= x_{k}$;
			\State Set $y_{k+1}= x_{k}+t^{k}_{R}*d_{k}$
			\State Evaluate $\xi_{k+1} \in \partial f (y_{k+1})$ 
			\State END NULL STEP		
		\EndIf
		\State Update $J_{k}$ and $u_{k}$ according some updating rules;
		\State $k=k+1$;
		\Until{\textbf{$v_{k}>\epsilon$.}}
		\EndProcedure 
	\end{algorithmic}
\end{algorithm}


\subsubsection{Algorithm convergence}
\label{PBM-Convergence}
The convergence of the \textit{Bundle method} is guarantee under specific assumptions as shown in theorem 7.16 in \cite{NonOpt}:
\begin{enumerate} \label{convergence:assBM}
	\item The function $f$ is convex;
	\item The set $X \in R^n$ is convex and closed.
\end{enumerate}

As mentioned in \cite{PaperBM} and demonstrated by Y. Du and A. Ruszczyński in chapter 4: "Rate of convergence" of \cite{CBM} ("Assumption 6: strong convexity of the function $f(\cdot)$") under the assumption that:
\begin{itemize}
	\item The function $f(\cdot)$ has a unique minimum point $x_*$;
	\item $\exists \alpha>0$, such that $f(x)-f(x_*)\geq \alpha \parallel x-x_*\parallel_{2},\forall x \in$ $\rm I\!R^{n}$ $\textnormal{ with } f(x)\leq f(x_1)$ 
	\\ (where $x_1$ are solutions of problems \ref{SMP} at the earlier iterations of the method).
\end{itemize}
  Then the convergence rate is linear. In this case, we suppose that given the number of iterations $k$, the precision of the solution is approximately $O(1/k)$. Without any assumption, the convergence rate is sublinear. 

The convergence theorem does not apply to our case, because our loss function is not convex. We think that this would be relevant in practice, but due to the \textit{Lipschitz continuity} of our loss function we suppose that the method can converge. 

In this direction, section 4: "Convergence Analysis" of \cite{PBM} helped us to understand that under specific assumption and constraint, the proximal bundle method for non convex function surely converge. In \cite{PBM} are defined the conditions for the convergence but it takes in consideration constrained problems. For this reason, we report the simplified assumption for unconstrained problems:
\begin{enumerate} \label{convergence:PBM}
	\item The function $f$ is $f^{\circ}$-pseudoconvex;
	\item The function $f$ is weakly semismooth;
\end{enumerate}

The definition of \textit{$f^{\circ}$-pseudoconvex} and \textit{weakly semismooth} are taken by chapter 2 of \cite{PBM}.\\
A function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is weakly semismooth if the classical directional derivative
\begin{equation}
f'(x,d)=\lim_{t\downarrow 0}\frac{f(x+t*d)-f(x)}{t}
\end{equation}
exists for all x and d, and
\begin{equation}
f'(x,d)=\lim_{t\downarrow 0}\xi(x+t*d)^{T}*d
\end{equation}
where $\xi(x + t*d) \in \partial f (x + t*d)$. 
\\
A function $f : \mathbb{R}^n \rightarrow \mathbb{R}$ is $f^{\circ}$-pseudoconvex, if it is locally Lipschitz continuous and for all x, y $\in \mathbb{R}^n$
\begin{equation}
f(y)<f(x) \, \textnormal{ implies } \, f^{\circ}(x;y-x)<0
\end{equation}
Where $f^{\circ}(x;y)$ is the \textit{Clarke’s} generalized directional derivative of f at x in the direction y and it is defined as
\begin{equation}
f^{\circ}(x,y)=\lim_{z\rightarrow x,t\downarrow 0}\frac{f(z+t*d)-f(z)}{t}
\end{equation}

We know that our function is globally and locally Lipschitz continuous and this is surely useful to prove that our loss function is $f^{\circ}$-pseudoconvex and weakly semismooth.
At the moment, our knowledge does not allow us to face an analysis to prove it. But, we think that the properties defined in section \ref{LF:Properties} can help to make it so. 

\subsubsection{Quadratic solver}
For the quadratic problem, we decided to use Gurobi solver \cite{Gurobi}. It uses the \textit{Barrier Method} to solve the quadratic problem. We decided to use it because it seemed well developed, fast and properly explained, also it exposes a well written C++ API which was appropriate for our project. In the aftermath, we think that it was the right choice because we have tried some other optimizers and we believe that, after learning its library, it was quite simple to model our quadratic problem.