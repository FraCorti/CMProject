\section{Experiments}

\subsection{Input data}
To test the correctness of the algorithms we used the MONK's datasets. To use these datasets correctly, we did the following steps:
\begin{itemize}
	\item We preprocessed MONK's datasets with \textit{1-of-k} encoding to convert categorical data to numerical data and we obtained 17 binary input features vectors. This preprocessing is divided between two classes, \textit{Preprocessing} and \textit{LoadDataset}. The former reads, shuffle, and splits the dataset whereas the latter performs the \textit{1-to-k} encoding. 
	\item To view all the networks in vector formulation terms and exploit the \textit{Armadillo} numerical library, we performed batch computation by loading and transposing the entire dataset in a single matrix. The labels were split and saved in another matrix to compute the \textit{MSE} (sez \ref{Loss:Mse}) after the forward phase. To reduce the cost of moving matrices we took advantage of the \textit{move} operator available since C++11. 
	\item Our library can deal with classification and regression tasks exploiting the composition of the \textit{Layer} class.  So we implemented \textit{sigmoid} and \textit{linear} activation functions for the output layer and \textit{hyperbolic tangent} activation function for the hidden layers.
\end{itemize}



To obtain a deterministic behaviour of the algorithms, we used the entire MONKS datasets as the input of the network. We obtained three matrices that had dimensions: 124x18 (Monk 1), 169x17 (Monk 2), and 122x17 (Monk 3). To compare the behaviour of the algorithms, we collected three parameters for every iteration: the error of the network (that is MSE for not regularized network and MSE plus the regularization term for regularized network), the norm of the gradient and the computational time spent on completing the iteration. These three parameters were used to make the rate of convergence, convergence speed, error and computational time plots shown below. For a better visualization of the plots the y-axis is plotted on a logarithmic scale (except for \S \ref{sec:rate_of_convergence} "Rate of convergence") and an enlargement version of each plots is put on the side. Before showing the plots a table with all the configurations used and the values obtained is shown.

\subsection{Configuration tested}
The notation $f^*$ specifies the optimal value reached by a specific configuration of the problem (that includes the regularization term if it is used). 

\begin{center}
	\small\addtolength{\tabcolsep}{-3pt}
		\centering
		\begin{longtable}{|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\textbf{Task}& \textbf{Optimizer}&\textbf{Model}&\textbf{Iteration} & \textbf{L rate} & \multicolumn{1}{l|}{\textbf{Lambda}} & \textbf{Mom} & \textbf{$f^{*}$}& \textbf{$\Vert \nabla f_{k}\Vert$ }& \textbf{Time(ms)}\\ \hline 
			Monk1 & MDA & M1 & 15000 & 0.9 & 3e-4  & 0.9 & 1.301e-2  & 7.855e-3 & 9161 \\
			Monk1 & MDA & M2 & 15000 & 0.9 & 3e-4  & 0.6 & 2.101e-2 & 6.740e-2 &  2321\\
			Monk1 & NMDA & M1 & 15000 & 0.9 & 3e-4  & 0.9 & 1.143e-2 & 8.343e-3 & 14695 \\
			Monk1 & NMDA & M2 & 15000 & 0.9 & 3e-4  & 0.6 & 1.105e-2 & 3.462e-2 & 5231 \\
			Monk1 & L-BFGS & M1 & 32 & 0.9 & 3e-4  & 0 &  7.939e-2 & 6.324e-6 & 19427  \\
			Monk1 & L-BFGS & M2 & 500 & 0.9 & 3e-4  & 0 &  8.916e-9 & 7.961e-6 & 1059 \\
			Monk1 & PBM & M1 & 2000 & 0.9 & 3e-4  & 0 & 8.796e-2  & 3.869e-5 & 1556359 \\
			Monk1 & PBM & M2 & 1314 & 0.9 & 3e-4  & 0 & 3.226e-2 & 1.627e-5 & 476750 \\
			
			Monk2 & MDA & M1 & 15000 & 0.9 & 3e-4  & 0.9 & 1.374e-2 & 1.236e-2 & 20422 \\
			Monk2 & MDA & M2 & 15000 & 0.9 & 3e-4  & 0.6 & 2.264e-2 & 9.747e-2 & 3421 \\
			Monk2 & NMDA & M1 & 15000 & 0.9 & 3e-4  & 0.9 &  1.374e-2 & 1.228e-2 & 18934  \\
			Monk2 & NMDA & M2 & 15000 & 0.9 & 3e-4  & 0.6 & 2.251e-2 & 1.264e-1 & 6098 \\
			Monk2 & L-BFGS & M1 & 670 & 0.9 & 3e-4  & 0 & 9.031e-2 & 9.438e-6 & 79945 \\
			Monk2 & L-BFGS & M2 & 500 & 0.9 & 3e-4  & 0 & 2.643e-8 & 4.929e-6 & 2432 \\
			Monk2 & PBM & M1 & 2000 & 0.9 & 3e-4  & 0 & 6.979e-6 & 2.376-4 & 1106435 \\
			Monk2 & PBM & M2 & 2000 & 0.9 & 3e-4  & 0 & 5.958e-6 & 2.647e-4 & 399099 \\
			
			Monk3 & MDA & M1 & 15000 & 0.9 & 3e-4  & 0.9 & 3.458e-2 & 7.675e-2 & 22280 \\
			Monk3 & MDA & M2 & 15000 & 0.9 & 3e-4  & 0.6 & 2.794e-2 & 3.502e-2 & 3458 \\
			Monk3 & MDA & M2 & 15000 & 0.9 & 3e-4  & 0.9 &  2.588e-2 & 1.504e-2 & 2846  \\
			Monk3 & NMDA & M1 & 15000 & 0.9 & 3e-4  & 0.9 & 1.587e-2 & 7.145e-3 & 20003 \\
			Monk3 & NMDA & M2 & 15000 & 0.9 & 3e-4  & 0.6 & 2.92e-2 & 3.045e-2 & 2738 \\
			Monk3 & NMDA & M2 & 15000 & 0.9 & 3e-4  & 0.9 & 2.618e-2 & 2.009e-2 & 3653 \\
			Monk3 & L-BFGS & M1 & 500 & 0.9 & 3e-4  & 0 & 7.792e-2 & 2.027e-5 & 73122 \\
			Monk3 & L-BFGS & M2 & 500 & 0.9 & 3e-4  & 0 & 8.196e-3 & 5.929e-6 & 1949 \\
			Monk3 & PBM & M1 & 2000 & 0.9 & 3e-4  & 0 & 1.229e-2 & 2.755e-4 & 270373 \\
			Monk3 & PBM & M2 & 2000 & 0.9 & 3e-4  & 0 & 8.199e-3 & 1.223e-4 & 265173  \\
			\hline
			\caption{Network configurations with $f^*$.}
			\label{tab:nets_res}
		\end{longtable}

\end{center}

The table \ref{tab:nets_comp} contains the configurations of the networks used to obtain the plots in \S \ref{sec:gradient_norm_convergence_speed}, \S \ref{sec:rate_of_convergence}, \S \ref{sec:computational_time} and \S \ref{sec:minimum_error}. 

\begin{center}
	\small\addtolength{\tabcolsep}{-3pt}
	\centering
	\begin{longtable}{|c|c|c|c|c|c|c|c|c|}
		\hline
		\textbf{Task}& \textbf{Optimizer} & \textbf{L rate} & \multicolumn{1}{l|}{\textbf{Lambda}} & \textbf{Mom}  \\ \hline 
		Monk1 & MDA & 0.9 & 0 & 0.9    \\
		Monk1 & L-BFGS & - & 0 & -    \\
		Monk1 & PBM & - & 0 & -    \\
		Monk2 & MDA & 0.9 & 0 & 0.9    \\
		Monk2 & L-BFGS & - & 0 & -    \\
		Monk2 & PBM & - & 0 & -    \\
		Monk3 & MDA & 0.9 & 0 & 0.9    \\
		Monk3 & L-BFGS & - & 0 & -    \\
		Monk3 & PBM & - & 0 & -    \\
		\hline
		\caption{Network configurations used for the comparison plot.}
		\label{tab:nets_comp}
	\end{longtable}
\end{center}


\subsubsection{Methods minima analysis} 

As mentioned in \S\ref{LF:convexity}, the objective function is not convex. For this reason, we try to understand if the methods used were approaching the same optimal value or different ones.
First of all, we analysed the $f^{*}$ value of each configuration to understand at which optimal values the algorithms converge. 

\begin{longtable}{|c|c|c|c|c|c|}
	\hline
	\centering
	\textbf{Optimizer}&\textbf{Iteration} & \textbf{L rate} & \multicolumn{1}{l|}{\textbf{Lambda}} & \textbf{Mom} & \textbf{$f^{*}$} \\ \hline 
	L-BFGS & 50 & - & 0  & 0 & 11e-15  \\
	PBM & 400 & - & 0  & 0 & 6.962e-6 \\
	NMDA & 900 & 0.9 & 0  & 0.9 & 1.124e-4  \\
	MDA & 900 & 0.9 & 0  & 0.9 & 1.159e-4 \\
	NMDA & 900 & 0.9 & 0  & 0.6 & 7.472e-4  \\
	MDA & 900 & 0.9 & 0  & 0.6 & 7.535e-4 \\
	NMDA & 900 & 0.9 & 0  & 0.3 & 2.426e-3 \\
	MDA & 900 & 0.9 & 0  & 0.3 & 2.458e-3 \\
	MDA & 900 & 0.9 & 0  & 0 & 8.678e-3 \\
	NMDA & 900 & 0.9 & 0  & 0 & 8.678e-3 \\
	L-BFGS & 50 & - & 3e-4  & 0 & 6.396e-2 \\
	L-BFGS & 50 & - & 5e-4  & 0 & 8.553e-2 \\
	L-BFGS & 50 & - & 7e-4  & 0 & 1.201e-1 \\
	PBM & 400 & - & 5e-4  & 0 & 1.352e-1  \\
	PBM & 400 & - & 7e-4  & 0 & 1.893e-1  \\
	PBM & 400 & - & 3e-4  & 0 & 8.114e-1  \\
	\hline
	\caption{Monk 1 optimizer configuration displayed in increasing order of $f^*$.}
	\label{tab:nets_res_plots_Monk1}
\end{longtable}

As can be seen from table \ref{tab:nets_res_plots_Monk1}, the different configuration of the optimizers MDA and NMDA with 900 iterations converge to the same optimal values. This can be stated because their optimal values differ between 3e-6 and 6e-6. Also for the L-BFGS optimizer with 5e-4 as regularization parameter using 50 iterations and the NMDA with 0.0 as momentum parameter using 900 iterations the optimal values obtained differ of 1e-3.

\begin{longtable}{|c|c|c|c|c|c|}
	\hline
	\centering
	\textbf{Optimizer}&\textbf{Iteration} & \textbf{L rate} & \multicolumn{1}{l|}{\textbf{Lambda}} & \textbf{Mom} & \textbf{$f^{*}$} \\ \hline
	L-BFGS & 50 & - & 0  & 0 & 7.6e-14 \\
	PBM & 400 & - & 0  & 0 & 3.453e-6 \\ 
	NMDA & 900 & 0.9 & 0  & 0.9 & 1.403e-4 \\
	MDA & 900 & 0.9 & 0  & 0.9 & 6.673e-4 \\ 
	MDA & 900 & 0.9 & 0  & 0.6 & 1.332e-2 \\ 
	NMDA & 900 & 0.9 & 0  & 0.6 & 1.380e-2 \\ 
	MDA & 900 & 0.9 & 0  & 0.3 & 2.374e-2  \\ 
	NMDA & 900 & 0.9 & 0  & 0.3 & 2.486e-2 \\ 
	L-BFGS & 50 & - & 3e-4  & 0  & 5.919e-2 \\ 
	MDA & 900 & 0.9 & 0  & 0 & 7.946e-2 \\ 
	NMDA & 900 & 0.9 & 0  & 0 & 7.946e-2 \\ 
	L-BFGS & 50 & - & 7e-4  & 0  & 8.965e-2 \\ 
	L-BFGS & 50 & - & 5e-4  & 0  & 1.156e-1\\ 
	PBM & 400 & - & 3e-4  & 0 & 1.211e-1 \\  
	PBM & 400 & - & 5e-4  & 0 & 2.018e-1 \\ 
	PBM & 400 & - & 7e-4  & 0 & 2.825e-1 \\ 
	\hline
		\caption{Monk 2 optimizer configuration displayed in increasing order of $f^*$.}
	\label{tab:nets_res_plots_Monk2}
\end{longtable}

As can be seen from table \ref{tab:nets_res_plots_Monk2}, the different configurations, with 0.3 and 0.6 as momentum parameter and 0.9 as learning rate, of the optimizers MDA and NMDA with 900 iterations differ of 9e-4 and 5e-4. Also the MDA and NMDA with 0 as momentum parameter obtained the same optimal values. In the end, for the L-BFGS with 5e-4 as regularization parameter and the PBM with 3e-4 as regularization parameter the optimal values obtained differ of 6e-2.  

\begin{longtable}{|c|c|c|c|c|c|}
	\hline
	\centering
	\textbf{Optimizer}&\textbf{Iteration} & \textbf{L rate} & \multicolumn{1}{l|}{\textbf{Lambda}} & \textbf{Mom} & \textbf{$f^{*}$} \\ \hline 
	L-BFGS & 50 & - & 0  & 0 & 8.196e-3 		\\
	PBM & 400 & - & 0  & 0 & 1.231e-2 			\\
	MDA & 900 & 0.9 & 0  & 0.9 & 1.254e-2 	\\
	NMDA & 900 & 0.9 & 0  & 0.9 & 1.254e-2 	\\
	MDA & 900 & 0.9 & 0  & 0.6 & 1.754e-2 	\\
	NMDA & 900 & 0.9 & 0  & 0.6 & 1.754e-2 	\\
	NMDA & 900 & 0.9 & 0  & 0.3 & 1.863e-2 	\\
	MDA & 900 & 0.9 & 0  & 0.3 & 1.864e-2 	\\
	MDA & 900 & 0.9 & 0  & 0 & 1.968e-2 		\\
	NMDA & 900 & 0.9 & 0  & 0 & 1.968e-2 	\\
	L-BFGS & 50 & - & 3e-4  & 0  & 5.781e-2 	\\
	L-BFGS & 50 & - & 7e-4  & 0  & 7.940e-2 	\\
	PBM & 400 & - & 3e-4  & 0 & 7.988e-2 		\\
	L-BFGS & 50 & - & 5e-4  & 0  & 8.555e-2 	\\
	PBM & 400 & - & 5e-4  & 0 & 1.249e-1 		\\
	PBM & 400 & - & 7e-4  & 0 & 1.699e-1 		\\
	\hline
		\caption{Monk 3 optimizer configuration displayed in increasing order of $f^*$.}
	\label{tab:nets_res_plots_Monk3}
\end{longtable}

As can be seen from table \ref{tab:nets_res_plots_Monk3}, the different configuration, with 0.0, 0.3, 0.6 and 0.9 as momentum parameter and 0.9 as learning rate, of the optimizers MDA and NMDA with 900 iterations differ of a maximum of 1e-5. In the end, for the L-BFGS with 7e-4 as regularization parameter and the PBM with 3e-4 as regularization parameter the optimal values obtained differ of 5e-4. 

After further analysis, we observed that the weights initialization influences the  optimal values reached by all the networks after the optimization process.

\subsubsection{Rate of convergence}
\label{sec:rate_of_convergence}
To compute the rate of convergence for each algorithm, we consider $f^*$ as the optimal value obtained by each of the optimization algorithm (that includes the regularization term if it is used) and $f(x_{k})$ as the optimal value obtained at the iteration $k$ plus the regularization term if it is used. If exists a real positive constant $\gamma$ such that
\begin{equation}
\label{eq:rofFirst}
lim_{k\to \infty}\frac{|f(x_{k+1}) - f^* |}{|f(x_{k}) - f^*|^p} = \gamma > 0,
\end{equation}
we can state that the sequence of values $f(x_{k})$ produced by the algorithm has an order of convergence $p$. In our case, we can derive an alternative faster and rapid procedure to compute it. Considering the equation \ref{eq:rofFirst} we can obtain the following formula: 
\begin{equation}
p = lim_{k\to \infty}\frac{log |f(x_{k+1}) - f^* | - log \gamma}{log|f(x_{k}) - f^*|} \approx lim_{k\to \infty}\frac{log |f(x_{k+1}) - f^* |}{log|f(x_{k}) - f^*|}.
\end{equation}
If $p=1$ the convergence is linear and if $p=2$ the convergence is quadratic. We can observe in figure \ref{CR-Monk2} that L-BFGS reached a superlinear rate of convergence. 
As we expect from the analysis we made, L-BFGS reached a superlinear convergence rate in some epoch. Also for the Momentum Descent Approach, we see that the convergence rate obtained is linear as we expected. Instead, for the Proximal Bundle method we had some convergences rate peaks that we didn't expect. In our opinion, this can be caused by the particular loss shape of Monks dataset.
It's interesting to view that all of these methods, even with a noisy dataset as Monk 3, converge with the same convergence rate as mentioned in the theory. The rate of convergence curves obtained, also with an enlargement for each of them put to the side, are shown below.

\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_CR_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_CR_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Converge rate comparison Monk 1 of configurations defined in table \ref{tab:nets_comp}}
	\label{CR-Monk1}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_CR_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_CR_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Converge rate comparison Monk 2 of configurations defined in table \ref{tab:nets_comp}}
	\label{CR-Monk2}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_CR_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_CR_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Converge rate comparison Monk 3 of configurations defined in table \ref{tab:nets_comp}}
	\label{CR-Monk3}
\end{figure}

\subsubsection{Computational time}
\label{sec:computational_time}
To compare the computational time behaviour of the implemented algorithm, we decided to display their curves in the plots \ref{CT-Monk1},\ref{CT-Monk2}, and \ref{CT-Monk3}. We can observe that, as expected from the theory, the most expensive method is the Proximal Bundle Method because of the time spent on the resolution of a quadratic programming problem by an external solver. As we can see in table \ref{tab:nets_res}, the computational time of the Momentum Descent Approach is always lower than the others: this means that it is faster in reaching a good value of the error in the Monks dataset. Moreover, we can infer that the L-BFGS algorithm gives us the best approximation of the objective function, with a greater cost for each iteration. This is due to the better descent direction given by the approximation of the Hessian. Also, for this approximation, it has to pay some high computational costs that usually do not allow to use this kind of approach in all the situation. The computational time curves obtained are shown below; an enlargement for each of them put to the side.

\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_CT_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_CT_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Computational time comparison Monk 1 of configurations defined in table \ref{tab:nets_comp}}
	\label{CT-Monk1}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_CT_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_CT_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Computational time comparison Monk 2 of configurations defined in table \ref{tab:nets_comp}}
	\label{CT-Monk2}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_CT_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_CT_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Computational time comparison Monk 3 of configurations defined in table \ref{tab:nets_comp}}
	\label{CT-Monk3}
\end{figure}

\subsubsection{Gradient norm convergence speed}
\label{sec:gradient_norm_convergence_speed}
Here we can observe that the behaviour of the algorithms follows what we stated before in the theory (\S \ref{Method}) about their convergence speed. We view from the plots that the norm of the gradient is not smooth in all the Monks dataset. In our opinion, this is due to the starting point of the training and the non-convexity of the objective function. We know from the theory that these methods tend to have some problems with the local minimum. Also, we can observe that at a certain point the norm tends to stabilize and converges to zero. Therefore, we can infer that in the neighbourhood of the last visited minimum there are no other better minima. For this reasons, the search of the best minimum can stop. The gradient norm curves obtained are shown below; an enlargement for each of them put to the side.
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_CS_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_CS_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Converge speed comparison Monk 1 of configurations defined in table \ref{tab:nets_comp}}
	\label{CS-Monk1}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_CS_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_CS_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	 \caption{Converge speed comparison Monk 2 of configurations defined in table \ref{tab:nets_comp}}
	 \label{CS-Monk2}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_CS_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_CS_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Converge speed comparison Monk 3 of configurations defined in table \ref{tab:nets_comp}}
	\label{CS-onk3}
\end{figure}

\subsubsection{Errors with respect to the minimum}
\label{sec:minimum_error}
The error with respect to the minimum for each iteration is obtained by the following formula: $ f(x_k) - f^*$ where  $f(x_{k})$ is the residual obtained at the iteration $k$ plus the  regularization term if it is used and $f^*$ is the optimal value reached by the specific configuration of the problem (that includes the regularization term if it is used). The plots of the error with respect to the minimum value obtained from the configurations in the table \ref{tab:nets_comp} are shown below, also an enlargement for each of them is put to the side.

An important thing to notice is that $f^*$ might not be 0 because it depends on the function that we minimize (that is only the MSE for not regularized network and the MSE plus the regularization term for regularized network). This function could be not convex and due to this fact, it could have multiple local minima in which we could stop. For this reason, to understand if an algorithm has converged to a local minimum, that cannot reach an error equal to zero, but it is still a minimum, the norm of the gradient has to be checked to understand how far from the stationary point (and how far from the end of convergence) we are. This can be seen in Monk 3 with L-BFGS optimizer without regularization in which the $f^*$ obtained is 8.198e-3 but the norm of the gradient is 1.9e-6, so the algorithm has converged near a local minimum.

\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_R_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk1/Monk1_R_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Errors with respect to the minimum comparison Monk 1 of configurations defined in table \ref{tab:nets_comp}}
	\label{R-Monk1}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_R_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk2/Monk2_R_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Errors with respect to the minimum comparison Monk 2 of configurations defined in table \ref{tab:nets_comp}}
	\label{R-Monk2}
\end{figure}
\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_R_Comparison_log_standard.png}
		\subcaption{Original}
	\end{minipage}%
	\begin{minipage}[t]{0.5\linewidth}
		\includegraphics[width=\linewidth]{data/Comparison/Monk3/Monk3_R_Comparison_log_zoom.png}
		\subcaption{Zoom}
	\end{minipage}
	\caption{Errors with respect to the minimum comparison Monk 3 of configurations defined in table \ref{tab:nets_comp}}
	\label{R-Monk3}
\end{figure}